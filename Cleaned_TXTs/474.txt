Assessing the corporate service quality performance of design-build contractors using quality function deployment
To cite this article: David Arditi & Dong-Eun Lee (2003) Assessing the corporate service quality performance of design-build contractors using quality function deployment, Construction Management & Economics, 21: 2, 175-185, DOI: 10.1080/0144619032000079716
To link to this article:
Full Terms & Conditions of access and use can be found at
Construction Management and EconomicsAssessing the corporate service quality of D/B contractors (2003) 21, 175–185
Assessing the corporate service quality performance of
Illinois Institute of Technology, Department of Civil and Architectural Engineering, Construction Engineering and Management Program, Chicago, IL 60616, USA
Received 13 March 2002; accepted 3 October 2002
The design–build (D/B) project delivery system has gained increased market share in the last few years. It is superior to traditional systems in some respects but some claim that it does not lend itself to effective quality assurance and control. D/B construction owners who are in the process of selecting a D/B firm for a project should therefore be well informed about the quality performance of potential D/B firms in their bidding list. Total quality consists of the corporate quality culture, the quality of the project service, and the quality of the constructed facility. This paper describes a tool that was developed to measure the corporate service quality of a D/B firm using quality function deployment (QFD). The service quality factors are ranked relative to construction owners’ needs and expectations by means of a survey administered to construction owners. The components of quality management systems in place in D/B firms are ranked by D/B executives in a survey administered to senior D/B firms. The relationships between service quality factors and quality system requirements are established by a professional quality system assessor and all these attributes are integrated into one system called the Corporate Service ‘House of Quality.’ The tool developed can be used by construction owners to rank D/B firms relative to corporate service quality as well as by D/B firms to benchmark themselves against their competitors.
Service quality, quality management, design–build contracts, quality function deployment
Introduction
In the last few years, the US design and construction community has witnessed a remarkable surge in the utilization of alternative project delivery systems. Flexible and single-source-authority delivery systems such as design–build (D/B) are gaining increased market share. Design–build is an integrated approach combining both design and construction in a single entity. It is a method of project delivery that facilitates innovative and flexible approaches such as phased construction, improves the ability to manage risk because there is a single point of responsibility, allows managers to take advantage of new materials and new technologies, and encourages the
*Author for correspondence. arditi@iit.edu
development of innovative practices that support energy efficiency and sustainability. While the D/B project delivery system has all these advantages over the traditional systems, it suffers from a major flaw in that the quality of the process and of the finished product cannot be guaranteed as the monitoring of quality is not as transparent as in the traditional general contracting or the construction management delivery systems.
Construction Management and Economics
ISSN 0144-6193 print/ISSN 1446-433X online © 2003 Taylor & Francis Ltd
D/B firms handle planning, conceptual, preliminary and detailed design, and procurement through construction to operation with sole responsibility for all these phases within their organization. The project that is delivered by a D/B firm can be defined as a temporary endeavour undertaken to create a unique product; the service the D/B firm provides through its corporate and project organizations is of paramount importance in the success of the endeavour. The constructed facility is the product that comes out of the process and that hopefully satisfies the requirements of the construction owner (i.e. the highway agency, the building developer, etc.) and of the end users (i.e. commuters on a highway, residents in a building, etc.). The service is generated by the D/B firm’s corporate and project organizations at the interface between the D/B firm and the construction owner. The evaluation of a D/B firm’s quality performance should therefore involve a systematic examination of (1) the extent to which the D/B firm is capable of delivering a high quality product that satisfies both construction owners and end-users, and (2) a high quality service provided to construction owners both at the corporate level and at individual project level.
The objective of the research reported in this paper is to develop a tool that measures the quality performance of D/B firms at the corporate level. Similar tools that measure D/B firms’ quality performance at individual project level and at product level have also been developed but are excluded from this paper for lack of space. Quality performance measurement tools may be used for the qualification, approval, registration, certification, or accreditation of D/B firms. They are also important for the self-diagnosis and continuous quality performance improvement of D/B firms.
In this research, service quality was measured in terms of the degree of satisfaction of customers’ (i.e. construction owners’) needs and expectations by assessing the performance of (1) the D/B firm’s quality management system and (2) the quality culture in place in the D/B firm. The service quality needs at the corporate level were identified; their relative weights and the strength of their relationships were investigated and prioritized in a survey administered to construction owners that have experience with D/B contracts. The quality management system components that are expected to be in place in a D/B firm were also identified; their relative weights and the strength of their relationships were investigated and prioritized in a survey administered to senior executives employed by D/B firms. The strength of the relationship between owners’ quality needs and the quality management system components were obtained from an independent quality system assessor with prior experience in D/B projects. The corporate quality performance measurement of D/B firms is conducted by performing quality function deployment (QFD).
Measuring quality performance
The foundations of the quality orientation of a company are defined at the corporate level. A corporate culture that encourages a quality conscious work environment promotes continuous quality improvement through values, traditions and procedures (Goetsch and Davis, 1997).
According to Evans and Lindsay (1996), quality conscious companies adopt quality management systems that focus not only on delivering high quality finished products but also on creating and/or sustaining performance improvement in the internal and external services generated by the company. However, because of the intangible nature of services, service quality is inherently more  difficult to measure than product quality. The perception of service quality results from a comparison of customer expectations with actual service performance. In their seminal paper, Parasuraman et al. (1985) proposed a conceptual model for measuring service quality that was based on the interpretation of qualitative data from extensive research performed in four service businesses. Their research revealed 10 dimensions transcending different types of services that customers use in forming expectations about and perceptions of services received.
Subsequent to the conceptual service quality model, Parasuraman et al. (1988, 1991) published a 22-item instrument referred to as SERVQUAL. Their research found that customers consider five basic dimensions in their assessment of service quality. However, the authors also concluded that the five basic dimensions could not be applied to all kinds of service environments. SERVQUAL has also been the subject of much criticism concerning its dimensionality and reliability (Carman, 1990; Babakus and Boller, 1992; Cronin and Taylor, 1992; Cronin and Taylor, 1994; Van Dyke et al., 1999). The consensus is that it would be best if each service business is evaluated starting with the original 10 dimensions. So, the most cautious approach to defining service quality dimensions in the context of D/B firms appears to be the adoption of the original 10 dimensions because:
(1) the vocabulary associated with these dimensions are easy to understand;
(2) these dimensions transcend services offered in various environments and therefore are applicable to D/B environments;
(3) dimensionality and reliability issues associated with SERVQUAL are eliminated.
The 10 service quality factors used in this study of D/B firms were adapted from the original 10 dimensions  identified by Parasuraman et al. (1985) and are presented in Table 1.
Strong concerns have been expressed about declining construction quality and customer satisfaction in the construction industry since the early 1980s (USACE Blue Ribbon 1983). In response to these concerns, extensive efforts were made in the last two decades to increase the overall quality of construction activities and these efforts are reflected in the many reports and papers published since then (e.g. Fox and Cornwell, 1984; Construction Technical Committee, 1987; Cost, 1989; Construction Industry Institute, 1989b; Oglesby et al., 1989;
Table 1	Service quality factors (modified from Zeithaml et al., 1990)
Service quality factors
Definitions
Consistency and dependability
Accessibility and convenience
Understanding the customer
The duration of the contract itself, including the time for mobilization and demobilization on site
The variation in the completion time of the contract compared to the scheduled date, including milestones
The number and value of the items on the punch list upon completion of the contract
The degree of respect, politeness, consideration and kindness of the design– build firm’s site and office personnel
The extent to which the design/build firm provides the same level of service performance to all clients at different times
The ease with which the contracting service is obtained from the Design– Build firm and approachability of the Design–Build firm for any problem
The ability to provide the right service at the first time with minimum amount of rework and the extent to which the service complies with owner’s requirements
The ability to react to the problems encountered during the project, the ability to withstand the variation of requirements during the project, and focus on meeting the client’s goals
The ability to disseminate information about the process of the project and to   listen to the owner
The ability that the design/build firm makes to understand the specific needs of each owner
Chase, 1993; Hindle and Rwelamila, 1993; Bubshait, 1994; Hart, 1994; Kubal, 1994; Ledbetter, 1994). The Malcolm Baldrige National Quality Award in the USA, the Deming Prize in Japan and the ISO 9000 international quality standards are the most prominent frameworks for quality management (Evans and Lindsay, 1996). They require company wide organizations to establish a well-structured and explicit system that identifies, documents, co-ordinates and maintains all the key quality related activities throughout all relevant company and site operations to ensure customer quality satisfaction and economical costs of quality. Methodical and autonomous evaluations called ‘quality audits’ are conducted to determine whether quality activities and results comply with planned arrangements and whether these arrangements are implemented effectively and are suitable to achieve objectives.
Since its introduction in 1987, the Malcolm Baldrige National Quality Award has become one of the most influential instruments for creating quality awareness throughout the world (Cook and Verma, 2002). The award’s Criteria for Performance Excellence establishes a framework for integrating total quality principles and practices into any organization. There has been however extensive discussion about the factors that affect quality performance at the intersection of hard and soft systems, i.e. engineers and mathematicians on the one hand and organizational scientists and psychologists on the other (Stenzel and Stenzel, 1998). For example, Black and Porter (1996) used the Malcolm Baldrige criteria as the baseline to develop a self-assessment framework that involved 10 critical factors that can better inform an organization in the development of its quality management system. Ahire et al. (1996) conducted similar studies examining and identifying the critical factors in total quality management. Wilson and Collier (2000) concluded in their study that the underlying theory of the Malcolm Baldrige programme supports the belief that leadership drives the system that causes business results. Curkovic et al. (2000) tested the assumption that the Malcolm Baldrige criteria adequately capture the major dimensions of total quality management. Because the Malcolm Baldrige National Quality Award is one of the most widely accepted models of performance excellence, the components of a corporate quality management system in a D/B environment were adapted from the Malcolm Baldrige criteria. The six components along with their brief descriptions are presented in Table 2.
Quality function deployment (QFD)
QFD is defined as ‘a technique to deploy customer requirements into design characteristics and deploy them into subsystems, components, materials and production processes’ (Hoyle, 1998). QFD is used in this research for translating the ‘voice of the construction owners’ through the various phases of project programming, planning, designing and construction into a final building product. The senior executives of D/B firms and an
Table 2 Corporate quality management system components (modified from Malcolm Baldrige National Quality Awards criteria)
Definitions

Information and analysis
The degree of encouragement by top management so as to lead to quality performance throughout the organization, the delegation of quality responsibility and authority to all levels of the design/build organization
The degree of importance that a firm places on client relationships and client satisfaction and the degree of knowledge about customers and the market
Collection, maintenance and use of information and/or data for measuring and improving quality performance. Analysis and review of company performance by analysing data collected within the organization, collection and use of comparative information to improve process of construction and the management of the organization
Identification of the needs of employee education, training, and development to achieve the organization’s success by incrementing the knowledge, skill, creativity and motivation of its workforce
Management of product and service processes, support processes and supplier and partnering process
Identification and evaluation of customer satisfaction results, financial and market results, human resource results, supplier and partner results, and company specific results
independent assessor of quality systems also contribute to the research as these parties are directly involved in administering and assessing corporate quality in D/B firms. The elements of the QFD ‘House of Quality’ are presented in Figures 1–4. The process of QFD involves five steps (Akao, 1990; Akao et al., 1994; Shillito, 1994; Zairi and Youssef, 1995).
identifying the elements and collecting the data
Figure 1 shows the data matrix that  contains information (IWi) about customer requirements (the WHATs), the technical characteristics (IHj) of the companies providing the service (the HOWs), and the strength of their interrelationships (Iij).
processing of the data in the data matrix
The information in column n and row m of the process matrix in Figure 2 represents the existing status (PWi and PHj) of the WHATs and HOWs in a company. They are evaluated and specified by an independent quality management system assessor on a scale of 1 to 5, where 1 is ‘poor’ and 5 ‘excellent.’
The boxed-in point scores (Rij) for each intersection between WHATs and HOWs are calculated by multiplying the mean of the relative importance of a HOW and that
of a WHAT by the strength of its relationships (Iij) specified in Figure 1.
where Rij is the point scores for each intersection between WHATs and HOWs; PWi is the status of each WHAT; PHj is the status of each HOW; Wi is the normalized weight of importance of each WHAT; Hj is the normalized weight of importance of each HOW; and Iij is the strength of the relationships between WHATs and HOWs (from Figure 1).
The importance ratings in the data matrix in Figure 1 are normalized, and add up to 1 in the process matrix in Figure 2. m
calculating the maximum achievable level of performance
The maximum level of performance (max LP) is achieved, if the existing status in all WHATs (PWi) and
in all HOWs (PHj) are rated as 5 (excellent).
The maximum level of performance (max LPi) for each WHATi is calculated as follows:
The maximum level of performance (max LPj) for each HOWj is calculated as follows:
The maximum level of performance (max LP) for a D/ B firm is calculated as follows:
Max LP constitutes the maximum achievable performance for the firm.
calculating the actual level of performance of a firm
It is likely that the actual levels of performance in WHATs and HOWs will take values between 1 and 5. The level of performance (LP) is calculated by using the same process used in Step 3.
The level of performance (LPi) for each WHATi, the level of performance (LPj) for each HOWj, and the level of performance (LP) for a firm, that is, the chart total, are calculated according to Eqs 4, 5 and 6, respectively. The quality performance index of the firm in question can be obtained from the following equation: Quality Performance Index =
LP from Step(	4)
×100%	(7) max LP from Step(	3)
Ranking firms according to their relative performance
Different firms may be ranked according to their relative performance. This can be called a ‘benchmarking’ exercise. In addition, if one wants to see how a company’s
Figure 3 Corporate service quality performance measurement model
performance is impacted by changes in its  performance status, one can perform what-if analysis.
Measuring service quality performance at the corporate level
The quality performance measurement model uses QFD and is shown in Figure 3. The details of the attributes and the data processing are as follows:
includes 10 ‘service quality factors’, which were modified from the 10 dimensions  identified in Parasuraman et al.’s (1985) conceptual
features normalized importance weights for quality management system components such that the condition set in Eq. 3 is satisfied.
represents the status of corporate quality management system components under perfect conditions (i.e. they all score a maximum 5).
the point scores (Rij) were calculated by the synthesis of the information in attributes , , ,  and  according to Eq. 1.
the maximum level of service quality performance at the corporate level under perfect conditions is calculated using the procedure defined in Eqs 4, 5 and 6.
represents the status of service quality factors under actual conditions in a particular D/B firm, as assigned by construction owners.
model developed after an extensive study of service industries. These factors represent customers’ quality requirements and their brief  descriptions are presented in Table 1. The relative importance rates of service quality factors were reported by construction owners in a questionnaire survey on a scale of 1 to 10 where 1 represents ‘not important’ and 10 ‘extremely important.’
includes six ‘quality management system components’, which were adapted from the Malcolm Baldrige National Quality Awards
(Inside the Baldrige Award Guidelines, 1993; Dale, 1994). They represent the technical characteristics with which D/B firms’ corporate activities are expected to meet D/B construction owners’ requirements. Their brief descriptions are presented in Table 2. The relative importance of quality management system components in place at the corporate level of D/B firms were reported by senior executives in D/B firms in a questionnaire survey on a scale of 1 to 10 where 1 represents ‘not important’ and 10 ‘extremely important.’
represents the strength of the relationships between the construction owner’s needs and expectations with respect to service quality factors (column ) and the quality management system components in place at the corporate level of D/B firms (row ). This information was obtained from a quality management system assessor (i.e., an independent third party) by means of a survey instrument on a scale of 0 to 5 where 0 represents ‘no relationship’ and 5 ‘perfect (one-on-one) relationship.’
features the normalized importance weights for service quality factors such that the condition set in Eq. 2 is satisfied.
represents the status of corporate quality management system components under actual conditions in a particular D/B firm, as reported by a quality management system assessor.
the point scores (Rij) were calculated by the synthesis of the information in attributes , ,  ,  and  according to Eq. 1.
the actual level of service quality performance at the corporate level under actual conditions is calculated using the procedure defined in Eqs 4, 5 and 6.
The participants who have an impact on service quality performance in D/B construction are identified based on their role, responsibilities, needs and expectations as: (1) construction owners; (2) senior executives of D/B firms; and (3) quality system assessors and/or consultants. Three sets of questionnaires were therefore prepared and administered to these three populations.
A survey questionnaire was administered to all 127 construction owners of D/B construction listed in the database of the Design Build Institute of America (DBIA). Four of the envelopes were returned by the Post Office because of wrong address. The rate of response for the remaining 123 mailings was 15.45%. The questionnaire sought information about the following: • The demographics of construction owners – this information was expressed in terms of company type (all were involved in D/B projects), years of experience in the industry (on the average 20 years), job title of the respondents (higher executives such as President, Vice President and Director of Public Works) and project type (mostly building construction).
• The relative importance of service quality factors – this information was sought for use in ‘House of Quality’ calculations. The values assigned by owners were later normalized in the process matrices. The service quality factors are defined in Table 1. These factors were modified from the factors used in Parasuraman et al.’s (1985) study.
A survey questionnaire was administered to all 126 senior executives of D/B firms listed in the database of DBIA.
Figure 5	Process matrix 1 – the maximum achievable level of performance
Five of the envelopes were returned by the Post Office because of wrong address. The name and address of D/B firms were obtained from the database of DBIA esign Build Institute of America). The rate of response for the remaining 121 mailings was 17.36%. The questionnaire sought information about the following:
• The demographics of senior executives in D/B firms – this information was expressed in terms of company type (all were D/B firms), company specialty (mostly building construction), years of experience in the industry (on the average 25 years) and title of the respondents (presidents and vice presidents).
• The relative importance of quality management system components at the corporate level – this information reflects the configuration of the quality management system in place at the corporate level, which has been pursued and implemented
by D/B firms. The quality management system components at the corporate level were extracted from the criteria used by the Malcolm Baldrige National Quality Awards. Their definitions are presented in Table 2.
Figure 6	Process matrix 2 – the actual level of performance of example D/B firm
A questionnaire was administered to a quality management system assessor who had experience with D/B construction. This type of respondent is hard to come by, as quality assessment of D/B firms is a rare occurrence. This person was the representative of one of the largest quality consulting and training organizations in North America. This company has been in existence since 1983 and has operated nationally and internationally with several permanent offices in North America and six other countries. Its client base includes a large variety of companies in various industries, including D/B firms. The person who answered the questionnaire was fully familiar with D/B projects. The information obtained included the strength of the relationships between quality management system components (i.e. the configuration of the corporate quality management system pursued by senior executives in D/B firms) and service quality factors (i.e., factors that owners use to set their quality needs and expectations).
The data collected through the three surveys administered to D/B construction owners, senior executives of D/B firms and the quality management system assessor with experience in D/B construction are presented in Figure 4. This data matrix corresponds to the matrix described in Step 1 in the methodology section, i.e. the data matrix in Figure 1 and the top matrix in Figure 3. In other words, the information in column  was obtained by means of the survey administered to construction owners; row‚ from senior executives of D/B firms; and matrix  from the independent quality management system assessor. Assuming that the maximum achievable performance status in each and every factor is a perfect 5, it is possible to make the calculations described in Steps 2 and 3. Hence, the process matrix 1 presented in Figure 5 corresponds to the middle matrix in Figure 3. Given the data collected in the three surveys described earlier, the maximum level of performance expected in the D/B  industry is 27.677 (bottom right corner cell in Figure 5).
As an example, let us now assume that a construction owner who wants to engage a D/B firm wants to assess the corporate quality performance of this firm. If the quality system assessors hired by the D/B construction owner rate the status of the quality management system components in place in the D/B firm being investigated (recorded in the status row of Figure 6) and the D/B construction owner defines his/her needs by assigning ratings (recorded in the status column of Figure 6) based on the particular project’s requirements, it is possible to make the calculations described in Steps 1 to 4 and produce the process matrix 2 presented in Figure 6; this matrix corresponds to bottom matrix in Figure 3. Given the status information for the particular firm in the example, the actual level of performance expected in the case of the D/B firm in question is calculated as 25.927 (bottom right corner cell in Figure 6). According to Eq. 7, which corresponds to the last step in Figure 3, the quality performance index of the firm in this example is obtained.
Quality Performance Index = Actual LP ×100% Max LP
Given the relationship in Eq. 7, it is clear that it is desirable for the performance index to be as close to 100% as possible. The real benefit of the performance index becomes apparent, however, when a construction owner compares the performance indexes of the bidding contractors. The performance index is also of value to individual contractors who can use it to compare their performance in different projects and take measures to maximize their Performance Index in future projects. They can also benchmark themselves against their competitors.
The corporate service quality performance measurement tool is designed as a relational database system using quality function deployment. The process matrices 1 and 2 described in Figures 3, 5 and 6 are calculated by means of a combined system designed as an Excel spreadsheet.

The purpose of quality performance measurement is not only to assess the quality performance of a D/B firm, but also to allow the D/B firm to benchmark itself against its competitors. The tool reported in this paper can, therefore, be used by a construction owner to accurately rank D/B firms with respect to corporate quality performance, as well as by D/B firms to conduct self-diagnosis, improvement, motivation, and training for achieving higher corporate quality standards. A ranking of D/B firms relative to quality performance can be invaluable to the construction owner who is in the process of deciding which D/B firm to pick for a particular project. It can also motivate D/B firms to strive for higher quality. Even though quality measurement is perceived as not being precise, a formal quality measurement system such as the one presented here makes the qualitative, invisible and abstract quality performance to be more visible, countable, manageable and quantitative.
This quality performance measurement tool is applicable only to D/B firms and projects, since the survey that investigates customers’ needs and expectations has been conducted only for this type of organization. In addition, the quality performance measurement has been conducted for overall quality system levels, not every stage in the quality loop, i.e., it does not distinguish between the design, construction and   maintenance phases. Future research should include the development of tools to measure project level quality performance and for the quality performance of the constructed facility.
