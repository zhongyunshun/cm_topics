Contractor ability criteria: a view from the Thai construction industry
To cite this article: Jakrapong Pongpeng & John Liston (2003) Contractor ability criteria: a view from the Thai construction industry, Construction Management and Economics, 21: 3, 267-282, DOI: 10.1080/0144619032000049647
To link to this article:
Full Terms & Conditions of access and use can be found at
Construction Management and Economicstractor ability criteria	 (2003) 21, 267–282
Contractor ability criteria: a view from the Thai construction industry
JAKRAPONG PONGPENG* and JOHN LISTON
Physical Infrastructure Centre, School of Civil Engineering, Queensland University of Technology, Australia
Received 8 February 2002; accepted 16 October 2002
Realizing that there is a lack of commonality in selecting criteria to evaluate contractor ability, the study aim was to develop a common set of contractor ability criteria for both government and private sectors. This included a standardized set of physical characteristics (hierarchical organizational units) of contractors. The Thai construction industry was surveyed as to the degrees of importance placed on a range of criteria and measures. Similarities and differences between the government and private sectors in selecting contractor ability criteria have been analysed by comparing the importance index and ranking order and comparing mean importance placed on criteria and measures. Relationships between all criteria and measures have also been explored by using correlation coefficients. Factor analysis has been applied to group all highly correlated measures together so as to develop a common set of contractor ability criteria. The result of analysing similarities and differences indicated only slight differences in the mean importance of criteria and measures between the government and private sectors. Thus, a common set of contractor ability criteria has been developed by applying factor analysis, namely, ‘engineering/construction’, ‘procurement/contract’, ‘project managers’, ‘human resources’, ‘quality management systems’, ‘health and safety’, ‘plant/equipment’, ‘financial strength’ and ‘public relations’.
Contractor ability criteria, contractor ability, Thai construction industry, prequalification, factor analysis
Introduction
In the construction industry, problems such as schedule delays, budget overruns, low quality work, a large number of claims and litigation, suffering of workmanship and the requirement of more supervision from the client result largely from not selecting the best contractor to complete a construction facility. These problems then affect the achievement of objectives, the day-to-day operations and the long-term performance of the client’s company.
To reduce these problems, tender evaluation processes were studied (Pongpeng, 2002). In that study, competitive bidding concepts underpinning tender evaluation were also reviewed. A questionnaire survey was conducted to investigate tender evaluation procedures used in the Thai
*Author for correspondence. kpjakrap@kmitl.ac.th
construction industry. A result of the survey shows that the selective tendering processes with and without prequalification processes use a two-step evaluation: step 1 evaluates contractor ability and step 2 evaluates tenders. In contrast, the open tendering process uses a one-step evaluation. That is, bid price and contractor ability are evaluated at the same time. All of these procedures involve multiple criteria. This reflects a move by the clients that the lowest bid is not the only criterion to select the best contractor. They sometimes trade off between bid price and contractor ability during selection (‘contractor ability’ concerns differences in how well contractors perform a project).
Construction Management and Economics
ISSN 0144-6193 print/ISSN 1446-433X online © 2003 Taylor & Francis Ltd
Although the development of criteria for the selection has been done by various researchers, a common set of contractor ability criteria, incorporating organizational units of contractors, for both government and private sectors still does not exist. In addition, different clients and different researchers suggest different criteria to evaluate contractor ability. For example, Liston (1994) suggested ‘past performance’, ‘business’, ‘capacity’, ‘financial’, ‘resource’, ‘procedure’ and ‘quality assurance’; whereas those by Hatush and Skitmore (1997) were ‘financial soundness,’ ‘technical ability,’ ‘management capabilities,’ ‘health and safety’ and ‘reputation’. In contrast, Holt et al. (1994) considered ‘contractors’ current workload’, ‘contractors’ past experience in terms of size of projected completed’, ‘contractor management resource in terms of formal training regime’, ‘time of year – weather’, ‘contractors’ past experience in terms of catchment’ and ‘experience in terms of projects completed’. This then results in a waste of researchers’ and clients’ repetitive resources in developing contractor ability criteria (cf, Hatush and Skitmore, 1997). To reduce the waste, there is a need for developing a set of contractor ability criteria that are common between government and private sectors.
This paper presents a study aimed at developing a common set of criteria with weights of relative importance to evaluate contractor ability (step 1) for government and private sectors, which is primarily developed on the basis of existing hierarchical organizational units of contractors (a physical feature of contractors). In this development, first data focusing on the degree of importance of criteria and their measures were gathered through a questionnaire survey within the Thai construction industry from 17 February 2001 to 23 March 2001. Participants were chosen from both government and private organizations, whose functions are involved with tender evaluation. Then, three main analyses have been performed to:
• determine similarities and differences in the selection of contractor ability criteria between government and private sectors using the comparison of importance index (mean/STD) and using hypothesis tests on mean differences of criteria and measures;
• examine relationships between all criteria and measures using Spearman rank correlation coefficients;
• apply factor analysis to group all highly correlated measures together.
The results of examining similarities and differences have revealed that both the government and private sectors consider similar criteria for evaluating contractor ability. Thus, a common set of contractor ability criteria can be developed for both the sectors. Subsequently, a result of factor analysis has suggested a common set of contractor ability criteria, namely, ‘engineering/construction’, ‘procurement/contract’, ‘project managers’, ‘human resources’, ‘quality management systems’, ‘health and safety’, ‘plant/equipment’, ‘financial strength’ and ‘public relations’.
Both government and private organizations involved with various construction works in Thailand were targeted. A hand-delivered questionnaire was given to a total of 210 construction professionals: 103 government and 107 private agencies.
Due to the difficulty of measuring contractor ability, a hierarchy of subcriteria was developed on the basis of the existing hierarchical organizational units of contractors:
• outline the criteria and measures according to contractors’ organizational units. (Based on a multilevel approach, any developing system should be primarily partitioned according to an organizational hierarchy (Dirickx and Jennergren, 1979).);
• review textbooks about necessary processes of organizational units of contractors (for more details, see Pongpeng, 2002);
• consider Thai standards for contractor registration of Bangkok Metropolitan Administration, Department of Accelerated Rural Development, Department of Highway, Public Works  Department and Royal Irrigation Department;
• review the relevant published works, for example, Construction Industry Development Agency (CIDA, 1993), Hatush and Skitmore (1997), Holt et al. (1994), Liston (1994, 1999), Russell
• synthesize the above to initially develop a common set of criteria for evaluating contractor ability; and • pilot the set of criteria with postgraduate students at the School of Civil Engineering, Queensland University of Technology and with practitioners in Australian and Thai construction industries to initially ensure validity including the exhaustive list of criteria and measures.
In assigning a scale to the criteria, a combination of a Likert scale and a bipolar adjective scale was used, 1 = very low importance to 5 = very high importance (for more details, see Pongpeng, 2002).
Sample characteristics
Types of organizations with their response rate are summarized in Table 1. The total rate of return was 68% (142). The government sector returned 79 questionnaires and had the highest return rate of 77% whilst the private sector returned 63 questionnaires at a return rate of 59%. This overall return rate is considered good as Babbie (1989) suggests that any rate over 50% can be reported, over 60% is good and over 70% excellent.
The sectors involved have a total annual contract value of A$24 932 million with the minimum and maximum values of A$0.01 million and A$10 000 million as shown in Table 2.1 In terms of annual average, the government sector has the higher contract value (A$306.9 million) compared to A$138.3 million for the private sector. The government sector engaged in various works totalling 20 459 contracts annually; whereas those of the private sector were 943 contracts. Clearly, the data covers a large number of contracts in various construction works with a large contract value from both government and private sectors.
Qualification analysis
In this study, the criteria together with their measures were developed based on the theory of hierarchy, multilevel, systems (see Mesarovic et al., 1970) and on considering Thai standards for contractor registration of Bangkok Metropolitan Administration (BMA), Department of Accelerated Rural Development (DARD), Department of Highway (DoH), Public Works Department (PWD) and Royal Irrigation Department (RID). In addition, the relevant published works was considered, for example, Construction Industry Development Agency (CIDA, 1993), Hatush and Skitmore (1997), Holt et al. (1994), Liston (1994), Russell and Skibniewski (1998) and Russell et al. (1992). The quality of the questionnaire was tested in terms of validity and reliability.
Table1 Sample characteristics in the Thai construction industry
Sector	OrganizationPercentage return

The Department of Accelerated Rural Development
The Department of Highways
The Royal Irrigation Department
The Public Works Department
The Electricity Generating Authority of Thailand
The Airports Authority of Thailand
The government organizations are large. They have a number of multiple decision-makers for tender evaluation.
Table2 A summary of characteristics of the respondents’ organizations


Civil works

Maintenance works
Other works

Sector	Approximate average annual contract	Contract values (A$M)
Bahts were converted to Australian dollars using the exchange rate of 23 Bahts/Dollar.
Validity means measuring what is expected to be measured. As a basis for discussion, the validity of any set of criteria/measures is open to criticism. No one set of criteria perfectly explain contractor ability (an abstract construct). All criteria of a set collectively explain contractor ability. Accordingly, they should correlate with one another in evaluating contractor ability (cf. Nunnally, 1967). Russell et al. (1992), for example, use
correlations between criteria for validation. Here too, correlation analysis was used to examine whether relationships between all criteria and their measures existed to ensure validity. The example results shown in Table 5 ensured that all criteria and measures were
correlated.
To ensure at a certain level that the scale (1–5) for measuring criteria/measures yields the same result over time, the internal consistency method was used. This method aims at finding the reliability coefficient based on the average correlation amongst criteria/measures (the internal consistency) and on the number of criteria/ measures. The basic formula for finding the internal consistency reliability is called coefficient alpha. In this study, Cronbach alpha was performed to test the internal consistency reliability of the criteria scale by solving (Nunnally, 1967):
where α is Cronbach alpha, N is the number of criteria/ measures (items) within a questionnaire, ∑σ2i is the sum of variance of each criterion/measure score and σ2x is the variance of a sum of all criteria/measures’ scores.
The alpha, as a reliability coefficient, varies from 0 to 1; the higher the alpha, the greater the internal consistency reliability or the greater the inter-criteria correlations. Here, the Cronbach alpha of 0.98 for the criteria scale indicated a good internal consistency reliability of the scale (the alpha should be greater than 0.7 (SPSS training, 1998)).
The data was analysed using the SPSS package. Three main analyses were performed: (1) finding similarities and differences between the government and private sectors using a comparison of importance index and comparison of mean importance; (2) revealing relationships between all criteria and their measures using correlation coefficients; and (3) clustering together all measures using factor analysis.
Test of similarities and differences
To find similarities and differences between government and private sectors, means and standard deviations of all criteria and their measures were explored. However, means may not fully represent the data if the data have high standard deviations. Thus, a standardized ratio (making the standard deviation equal 1) of the mean and standard deviation was constructed for the use of comparative purposes, which was written as (cf. Lehmann, 1989):
Importance index = Mean Standard deviation
Moreover, to draw a conclusion as to whether government and private sectors consider criteria differently as they evaluate contractor ability, the mean importance of each criterion and measure was compared using the Mann Whitney U test.
Comparison of ranking order and importance index across sectors
For a descriptive analysis and the sake of readability, the five most important criteria and measures were compared. A summary of the comparison is presented in Table 3. Overall, the five most important criteria were: (1) ‘project planning’, (2) ‘project monitoring’, (3) ‘project management experience’, (4) ‘ability to adjust a project’ and (5) ‘performance’. The five most importance measures were (1) ‘master plans’, (2) ‘continuously reporting’, (3) ‘a list of plant/equipment’, (4) ‘past performance’ and (5) ‘problem-solving skills’.
Clearly both sectors considered ‘project planning’, ‘project monitoring’ and ‘project management experience’ as very important. ‘Performance’ and ‘ability to adjust a project’ were indicated by the government sector as important. A possible reason is that these two criteria largely affect the timeliness of a project, which guarantees that the fiscal year budgeting requirements are complied with. On the other hand, the private sector indicated ‘subcontractor control’ and ‘financial ratios’ as important possibly because these two criteria extensively affect the cost of a project. This, in turn, ensures economic viability of private organizations in business.
It is interesting to look at the criteria on quality and safety. As shown in Table A1 in the Appendix, both sectors rated criteria ‘quality system selection’ (importance indices of 2.94 and 3.64 by public and private sectors, respectively), ‘quality system implementation’ (importance indices of 2.98 and 3.60) and ‘quality system audits’ (importance indices of 3.44 and 4.32) as being of medium-to-high importance. Based on these results, the quality system selection, implementation and audits are not of high concern for either sector. On the other hand, although health and safety performance can block the execution of a project and lead to an additional cost to a project, it is rated as semi-important by the government sector (an importance index of 3.56) but as rather important by the private sector (an importance index of 4.79). This reinforces the belief that any criteria possibly affecting project cost are most likely to be of great importance to the private sector.
Table3 Comparison of the five most important criteria and measures

Performance
Performance
Continuously reporting
Past performance
Past performance
Continuously reporting
Technical ability
In the measures shown in Table A2, only ‘master plans’ was indicated as highly important by both sectors with the government sector placing a higher priority (ranked first) than that (ranked third) of the private sector. This again shows the possible underlying philosophy that time tends to be of more concern to the government sector. In contrast, the private sector ranked ‘budgeting’ as highly important (ranked second). A  possible explanation is that this measure helps to establish financial viability. However, the government sector ranked ‘budgeting’ 26th, which again highlights the possible underlying philosophical differences between the government and private sectors.
Other measures amongst the five most importance rankings for the government sector were ‘past performance,’ ‘continuously reporting,’ ‘a list of plant/equipment’ and ‘problem-solving skills’. The possible reason why the first three measures are considered as important is that all these measures are prescribed by most government sectors’ control manuals and cannot be easily breached. Also, having ‘problem-solving skills’ as a project manager is of major concern perhaps because most government organizations require contractors who have the ability to solve their own problems (e.g. allocating project resource requirements, surviving company constraints and managing risks associated with the project) and to correct errors/mistakes that may occur in the specifications and drawings without having to request solutions from the superintendent.
For the private sector, the remaining five most important measures were ‘technical ability’, ‘observation skills’ and ‘analysis skills’. A possible explanation why these three measures are important is that the private sector wants sub/contractors that have high technical ability, show basic technical knowledge and understand construction projects. Also, the contractors tend to require a project manager who has good communication skills and is therefore able to effectively and efficiently deliver plans, controls, tasks and standards to other colleagues, helping to achieve the completion of the facility to a pre-specified budget.
Of interest are measures describing quality, and health and safety. Both the public and private sectors rated ‘AS 3900 series’ as being of low importance (1.85 and 1.86, respectively), whereas ‘ISO 9000 series’ was of medium importance (2.54 and 3.03, respectively). The comments from some respondents were that they were more familiar with standard ISO than joint standard AS/NZS. ‘Progressive steps of implementing a quality system’ was rated as being of medium importance by the public sector (an importance index of 3.09), but of rather high importance by the private sector (an importance index of 3.98).
Another interest was that ‘documented processes being followed by contractor,’ ‘documented processes in place ready to address standard elements,’ and ‘documented processes being effective and suitable’ were rated as being of ‘medium’-to-’high’ importance (range between 3.16–3.53) by the government sector, and as ‘ratherhigh’-to-’high’ (range between 3.76–4.15) by the private sector. Also ‘health and safety plan’ and ‘health and safety control’ were rated as being of medium importance (range between 3.06–3.16) by the government sector and as ‘rather-high’-to-’high’ importance (range between 3.95–4.08) by the private sector.
The comparison of ranking order between government and private sectors has shown that three out of the five most important criteria, 60%, are selected in agreement. However, only one out of the five most important measures (‘master plans’, 20%), is similarly selected. In addition, when importance indices of criteria and measures on quality and health and safety are compared, the overall statistical figures show that the government sector places a lower priority on these criteria (importance indices of 3.23 and 4.09 by government and private sectors, respectively) and measures (importance indices of 3.26 and 3.97 by government and private sectors, respectively) than does the private sector. Furthermore, as discussed earlier, the five most important criteria and measures indicated by the government sector are likely to be time-related whilst those of the private sector are likely to be cost-related. Nevertheless, at this stage it cannot be concluded whether the government and private sectors consider criteria and measures differently as they evaluate contractor ability. To explore this further, hypotheses on which criteria and measures make the two sectors different at specified statistical levels were tested.
Hypothesis test
In conjunction with the previous section, differences and similarities in characteristics between the two sectors were further inspected. A nonparametric statistical test, Mann Whitney U test, was performed to compare mean importance of each criterion and measure as to whether there was any statistical difference at the 95% level of confidence.
The aim of the Mann Whitney U test is to draw a conclusion on the existence of mean differences of variables between two population groups which are selected independently (for more details, see Seigel and Castellan, 1988; Keller and Warrack, 1997). The test was done against the null hypothesis: (H0) the mean importance of each criterion and measure are equal for both the government and private sectors.
Table 4 summarizes the result (listed only the criteria and measures indicated as statistical differences between government and private sectors). In the table, only five out of 23 criteria, 22%, were indicated as statistically different in terms of mean importance, namely, ‘financial ratios’, ‘quality system implementation’, ‘project execution’, ‘communication skills’ and ‘adaptability’. Also, only 19 out of 63 measures (30%) were statistically different, including ‘gross profit’, ‘progressive steps of implementing a quality system’, ‘competitive incomes/ welfare’, ‘master plans’ and ‘budgeting’.
It can be seen from Table 4 that less than 31% of the number of criteria and measures are indicated as statistically different at the 95% level of confidence. This means that more than 69% of the number of criteria and measures are similarly selected by government and private sectors. Therefore, it can be inferred that both government and private sectors consider criteria and their measures rather similarly (69%) in evaluating contractor ability.
Relationships between all criteria and measures
Table4 Criteria and measures indicated as statistical differences between government and private sectors
Measures indicated as statistical difference*
Quality system implementation
Communication skills
Current banking organization
Average length of time that the contractor pays subs/suppliers
Conditions in bank guarantee
Progressive steps in implementing a quality system
Competitive incomes/welfare
General conditions of subcontractors
Contingency plans
Communication of the plans to involved people
Technical ability
Management of conflict
Previous and current position
Tolerance for ambiguity
To identify that the criteria and measures were valid and relevant to the evaluation of contractor ability, the relationships between them were examined. The Spearman Rank Correlation method was selected to calculate the
correlation coefficient, r, since normality of population is not required (Seigel and Castellan, 1988). The coefficient ranges between −1 and +1 where −1 and +1 indicate high relationships but 0 does not.
The coefficient was used to test the null and alternative hypotheses: (H0) that there is no relationship between the two criteria or the two measures and (Ha) that there is a relationship between the two criteria or the two measures. The level of confidence for the test was 95% or 99%. Almost all criteria and their measures were correlated except the criterion ‘quality system selection’ to its measure, ‘AS 3900 series’.
Criteria considered as having high and low importance are shown in Table 5. The statistically significant relationships between the criteria and their measures are indicated by * (at the 5% level of significance) or ** (at the 1% level of significance). Clearly ‘financial ratios’ and ‘banking arrangement’ had the weakest relationships with other criteria. ‘Delivery control’ was strongly correlated to ‘subcontractor control’, as was ‘project planning’ and ‘project monitoring’. Similarly, there was a strong relationship between ‘project monitoring’ and ‘ability to adjust a project’. Furthermore, in an observation of the correlation figures, the measures were likely to separate into clusters because the measures of a cluster correlated highly with one another and correlated much less with the other measures of other clusters. This gives a convincing evidence for applying factor analysis to group highly correlated measures together.
In the overall section, criteria and measures were correlated. This confirms the relevance of all the selected criteria and their measures to the evaluation of contractor ability.
Factor analysis
Factor analysis was used to group highly correlated measures together and then to find weights of relative importance amongst those groups. The two basic reasons for the grouping are (1) the simplification of modelling problems and (2) the exploration of the underlying structure of measures, particularly whether the structure is compatible with hierarchical organizational units. The details of factor analysis can be seen in Aaker et al. (1998) and Lehmann (1989).
A condition required by the factor analysis model is that the number of observed samples must be greater than the number of measures. For this reason, any measures having an importance index of less than 3 (considered as of medium importance) were removed. Accordingly, only 53 measures were used as input for factor analysis as shown in Table A3 in the Appendix.
Table5 Spearman rank correlation coefficient of ten example criteria
Credit Procureratings ment plans
Delivery control
Credit ratings
Delivery control
*at the 5% level of significance; **at the 1% level of significance.
The coefficient values indicated that all measures were correlated (Table 5) and testing indicated that the data were appropriate for using factor analysis. The tests were (1) the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy and (2) Bartlett’s test of sphericity. The KMO measure of 0.897 indicated that the data were adequate for using factor analysis. Also, the Bartlett’s
test indicated that relationships amongst measures were suitable for running factor analysis.
Factor extraction
The Principal Components were used to extract the number of measures. What the Principal Components does is to combine many correlated measures into a small number of components, namely:
• the first component, which contains the maximum information in all the measures. This component has the largest variance that can explain the problem most effectively;
• the second component, which is independent of the first component, and contains as much of the remaining information in all the measures as possible, and so on.
The result from examining the greater-than-one eigenvalues of the principal components suggested 12 components to retain as shown in Table A3 in the Appendix. Each row of the table contained factor loadings that indicated which measures belong to each component. The values of factor loadings were still not clear in indicating which measures should belong to which components because the values of factor loadings of many measures on each component were close. In order to develop a clear pattern, a modification of the factor loadings by rotating the principal components was necessary.
Another important result of this step is the total variance explained by each component. Table 6 summarizes the result. The 12 components together accounted for 75% of variance of all the measures. The first component explained 39% of variance of all the measures whilst that of the last component explained only 2%.
Factor rotation
The rotation of the components was performed to adjust
Extraction sums of squared loadings (eigenvalues)
Rotation sums of squared loadings
Percentage of variance
Percentage of variance
the values of factor loadings so that the new values were closer to −1, +1 or 0. These new values make the grouping of measures easier if each variable has a high factor loading (close to −1 or +1) on a single component, but small factor loadings (close to 0) on the other components.
The orthogonal rotation using the varimax method was performed. This rotation made it easier to clearly identify which measures belong to which components. Retaining 12 components was still suggested; the results are shown in Table 6. However, the prior theory (i.e. the theory of hierarchy, multilevel, systems) being applied to the problem suggests that the selection criteria should be selected according to existing organizational units of contractors. Based on a combination of this and the prior theory, nine components were adopted and named as shown in Table 7.
Also in Table 7, the percentage of variance explained, factor loadings and their normalized weights of relative importance are summarized. The total percentage of variance of the nine criteria was 69%, indicated as acceptable intercorrelation. The criteria ‘engineering/ construction’ accounted for the most variance (39%) followed by ‘procurement/contract’ accounting for 7%. Surprisingly, ‘financial strength’ accounted for only 3% of the variance. One possible reason is that most contractors can use outsourcing funds to run their businesses. The structure of a common set of contractor ability criteria with their normalized weights of relative importance is shown in Figure 1.
Table7 Percentage of variance, factor loading and normalized weights
Criteria and measures
Percentage of variance
Normalized weight (%)
(1) Engineering/construction
Delivery control
(5) Quality management systems Quality system implementation
Quality system audits
(6) Health and safety
Occupational health and safety
Credit ratings
(9) Public relations
Performance
This study attempts to find a common set of contractor ability criteria for government and private sectors, which includes the physical property of contractors (i.e. organizational units of contractors). Three main analyses focusing on the degree of importance of criteria and measures were performed. The first analysis determined similarities and differences in the selection of criteria between government and private sectors using the comparison of importance index (mean/STD) and using the Mann Whitney U method to test hypotheses on mean differences of criteria and measures. The results revealed that both government and private sectors considered similar criteria as they evaluated contractor ability, which infer that if the criteria are developed consistent with organizational units of contractors, types of clients do not affect the selection of criteria. This then suggests that a common set of contractor ability criteria can be developed by applying factor analysis. The second analysis examined relationships between all criteria and measures, which is aimed at validation of them. Overall, the result showed that criteria and measures were correlated. In addition, the observation of correlations revealed that the members (measures) should be grouped into clusters because the members of a cluster correlate highly with one another and correlate much less with the members of other clusters. This again provides persuasion to apply factor analysis. The third analysis applied factor analysis to group to highly correlated measures together. As a result, a common set of contractor ability criteria have been developed, namely, ‘engineering/ construction’, ‘procurement/contract’, ‘project managers’, ‘human resources’, ‘quality management systems’,
‘health and safety’, ‘plant/equipment’, ‘financial strength’ and ‘public relations’. The theory of hierarchy, multilevel, systems led us to infer that the contractor ability criteria should be developed to correspond with existing organizational units of contractors. The results of the factor analysis confirmed this inference. One possible reason is that a common characteristic of all contractors is the existence of their organizational units, which, in general, structure an organization. The commonality then lead to the development of a common set of contractor ability criteria. Although the differences of organizational units between contractors may exist, similar necessary functions of contractors are performed to operate their businesses. Thus, this reason is still valid. Lastly, the results of this paper have provided a starting
position for development of a realistic working model in tender evaluation.
Acknowledgements
The authors appreciate the Thai construction industry participants’ attempts to complete the questionnaire. Also, the authors wish to thank the Physical Infrastructure Centre, Queensland University of Technology for supporting travelling expenses to gather the data in Thailand. Our thanks are also extended to the anonymous reviewers for their invaluable comments.
