Construction Management and Economics (January 2006) 24, 31–44
Conceptual framework for assessment of client needs and satisfaction in the building development process
JASPER MBACHU and RAYMOND NKADO*
Faculty of Engineering and the Built Environment, University of the Witwatersrand, Wits, South Africa
Received 15 April 2004; accepted 17 January 2005
A conceptual framework is developed for assessment of client needs, and the measurement and monitoring of client satisfaction levels in the building development process. Data were obtained from qualitative and quantitative surveys of a target population of clients of commercial buildings in South Africa. Satisfaction levels based on multi-attribute measures were compared with those based on single evaluative responses, using Wilcoxon’s matched-pair test. Results showed no significant differences in pairwise comparisons. A strong positive correlation also existed between both equivalent measures of client satisfaction levels. These results validate the conceptual framework. Results of evaluation of client satisfaction levels showed that clients perceived average levels of satisfaction in the building development process. Areas for improvement in the services of contractors and consultants were identified through ‘Criticality Index’ analyses. Empirical models were developed for proactive measurements of client satisfaction levels at distinct stages of the development process. A dynamic approach to satisfaction measurement is recommended. This contrasts with post-purchase and static views adopted in the consumer services segment and enables consultants to monitor and improve satisfaction levels proactively, as the development process evolves.
Building development, criticality index, needs assessment, performance measurement, satisfaction measurement
Introduction
The concept of customer satisfaction is largely developed in the production sector and consumer services markets, and is regarded as the raison d’eˆtre for companies’ existence and operations (Taylor and Baker, 1994). The provision of service or production of a product offered for sale should be aimed at satisfying identified needs of the targeted customers (Rust and Zahorik, 1993). This is because client satisfaction adds value to the organization from a number of perspectives: creation of sustainable client loyalty to the firm (Preece, 2000), repeat purchase, acceptance of other products/services offered by the service firm, increased market share and profitability levels (Surprenant, 1977), creation of positive word-ofmouth (Churchill and Surprenant, 1977) and a measure of market performance (Handy, 1977). Satisfaction is also important to the client because it
reflects a positive outcome from the outlay of scarce resources and/or fulfilment of unmet needs (Day and Landon, 1977; Landon, 1977). On the other hand, consumer dissatisfaction leads to undesirable consequences such as: negative word-of-mouth, complaints, redress seeking, reduction of market share and profitability levels (Day, 1977; Oliver, 1981; Woodruff et al., 1983) and possible divestment from the industry (Kotler, 1997).
The foregoing suggest that a study of client needs and satisfaction is crucial, as current and future prospects in the construction industry depend on the extent to which clients are satisfied with their investments in the procurement process. It is therefore disheartening to read about the rising spate of client dissatisfaction in the industry. Although dated, the NEDO (1974) reports that UK clients were dissatisfied with their buildings. More recently, Bowen et al. (1997) posit that, ‘the construction industry potentially has a higher proportion of dissatisfied and critical clients than
any other industry’ (p.1). This agrees with an earlier
Construction Management and Economics
ISSN 0144-6193 print/ISSN 1466-433X online # 2006 Taylor & Francis
observation by Kometa et al. (1994), that ‘evidence abounds to suggest that clients are largely misunderstood and dissatisfied with the performance of their consultants and contractors’ (p.433).
Previous studies have identified several factors responsible for client dissatisfaction in the construction industry. For instance, Nkado and Mbachu (2001), while documenting causes of client dissatisfaction in the procurement process note, among other factors, the issue of dissonance between objective reality and client’s perceptions of it, adding, as a lesson for professionals, that client satisfaction/dissatisfaction is a subjective phenomenon, which may not be based on objective reality (e.g. delivery of the project within time, cost and quality targets), but on client’s perceptions of the objective reality. These perceptions are influenced by several subjective dimensions, which could cause significant dissonance between what clients perceive of the target being observed (e.g. project team performance) and the reality about the observed target (e.g. meeting project schedule). Dissatisfaction could result when the project team focuses on objective reality rather than seeing things from clients’ perspectives.
Another factor responsible for client dissatisfaction in the construction industry is the possibility of clients’ stated requirements not sufficiently addressing their real (latent) needs. Owing to insufficient time for in-depth viability appraisal, clients usually adopt irrational approaches in making decisions concerning optimal solutions to their real needs. This results in divergence between clients’ stated and real needs in the procurement process. On the basis of this, Goodacre et al. (1982) argue that listening to, and acting only upon, client’s stated needs may not yield the desired benefits. That this has been the case in the construction industry could be responsible for the reported cases of client dissatisfaction.
Furthermore, the prevalence of client dissatisfaction in the global construction industry has been attributed to inadequate research into client needs and satisfaction. For instance, Liu and Walker (1998) submit that not much effort has been made to identify the needs of clients, which is crucial to ensuring client satisfaction. Green and Lenard (1999) corroborate this by noting that, as a recurring problem throughout the global construction industry, the industry has invested little time and attention in investigating the needs of its clients compared to other economic sectors.
Perhaps the construction industry’s service providers have been unable to fully grasp the issue of client satisfaction largely because of the absence or unawareness of a mechanism for measuring satisfaction in the procurement process. This could be inferred from Bowen’s (1993) findings that ‘little published literature exists on the appropriate mechanisms for assessing client satisfaction’ (p.60).
These developments indicate that further research is needed in the area of client needs and satisfaction within the construction industry. The concepts of assessment of customer needs and satisfaction are fully developed and operational in the production and consumer services sectors, and could be adapted and applied to the construction industry to improve client satisfaction levels. This study could contribute to filling the gap by providing the mechanism for client needs assessment and satisfaction measurement in the development process.
The objective of this paper is to develop a conceptual framework for a dynamic assessment of client needs and satisfaction based on prioritized expectations of clients from the services of consultants and contractors in the development process.
Clients’ needs and requirements in the development process could be categorized broadly into design (architectural and engineering), management (construction project and cost) and construction services, in line with Bennett’s (1985) four major areas of responsibility in construction project development. A framework for client needs assessment in the development process as proposed in this study focuses on the identification and prioritization of client expectations from design, management and construction services from the perspectives of clients. This is illustrated in Figure1.
The above conceptual framework underpinned the research design for this study, whereby investigations were undertaken to determine constructs underlying each of the above five dimensions during the exploratory surveys, while the quantitative surveys were aimed at prioritizing the identified constructs from clients’ perspectives.
Measurement of satisfaction
Kotler (1997) defines satisfaction as ‘a person’s feeling of pleasure or disappointment resulting from comparing a product’s perceived performance or outcome in relation to his or her expectations’ (p.40).
Figure 1	Conceptual framework for client needs assessment in the development process (Source: Mbachu, 2003)
Many researchers consider satisfaction as an overall summary measure, while others feel that satisfaction is measured best by a combination of facets or attributes. For instance, Day (1977) sees no difficulty in measuring an individual’s satisfaction or dissatisfaction with the overall outcome. Also, Czepiel and Rosenberg (1977) agree that consumer satisfaction can be thought of as a single overall evaluative response that represents a summary of subjective responses to many different facets. Handy and Ptaff (1975), however, disagree with overall satisfaction measurement, arguing that response to an overall satisfaction measure only crudely measures overall satisfaction. Zikmund (1994) corroborates Handy and Ptaff’s (1975) views by contending that measures of cognitive phenomena (such as satisfaction) are often composite indexes of a set of variables.
This research draws on the views of Handy and Ptaff (1975) and Zikmund (1994) on the appropriate approach to the measurement of satisfaction. This involves the combination of facets or attributes, the measurement of which would give the overall satisfaction level. However, measurement of satisfaction on the basis of a single observation was also explored to provide an alternative index measure for comparative analyses. Assuming that both multi-attribute and single evaluative measures of client satisfaction are valid, the alternative index measure serves to assess the internal consistency (Zikmund, 1994) of the findings and methodological approach adopted in this study.
Theoretical satisfaction models
Two models for satisfaction measurement adopted in the study are highlighted below.
Satisfaction level based on single evaluative (overview) responses (So)
This is based on Czepiel and Rosenberg’s (1977) summary view of satisfaction measurement. It is computed as the analysed performance index (PI) value of clients’ overview ratings of a professional group’s performance, without considering performances in each identified need or requirement in the group’s service. It is used as an alternative index measure for comparing satisfaction scores with a view to assessing the internal consistency (Zikmund, 1994) of the developed conceptual framework as proposed in this study. The analysis of satisfaction score (So) from clients’ ratings of performance is shown in equation1.
Satisfaction level based on multi-attribute measures (ST)
This is based on Handy and Ptaff’s (1975) view of composite index measure of satisfaction. It is calculated as the sum of satisfaction scores (Ss) in a given subset. It gives an indication of the level of satisfaction derived by clients from the services of a particular professional group on the basis of perceived levels of importance of a set of needs or requirements, and the professional group’s performance in meeting the needs.
The proposed theoretical model for the multiattribute estimation of the level of client satisfaction (SO) in the development process is of the generic form:

SQ5level of satisfaction with quantity surveying services, comprising satisfaction (Sqi) with a number of attributes (ranging from 1 to M) perceived by the client to be relevant in line with his or her priorities. SA, SE, SM, SC represent levels of satisfaction with architectural, consulting engineering, management and construction services, respectively. A, B, C, D and E are the relative weights assigned to the respective services in accordance with client’s perceived levels of importance attached to the service components in the satisfaction continuum.
The aim of the qualitative surveys was to explore the variables underlying the subcomponents of equation2. The aim of the quantitative surveys was to obtain data, the analysis of which could yield the relative weights or constants in each model subcomponent of equation2.
The descriptive survey method was adopted in the study as the technique of observation was employed in the gathering of data through the benefit of questionnaires (Leedy and Ormrod, 2001). The respondent target population comprised commercial property clients in South Africa who are registered members of the eminent South African Property Owners
Association (SAPOA), and operate as property developers, investors or owner-occupiers.
The data gathering process involved semistructured pilot interviews conducted with 15 directors of client organizations within the target population in Port Elizabeth and Johannesburg – two of the major cities in South Africa. The constructs generated at this qualitative survey stage were used in designing a questionnaire, which was pretested and administered nationwide to the SAPOA members who did not participate in the interviews and pre-tests. The surveys were conducted between October 2001 and November 2002. However, no time limitation was placed on the experiences of respondents.
Methods of data analysis
Multi-attribute methods (Zikmund, 1994; Cooper and Emory, 1995) were employed in the preliminary analyses of data to establish the mean ratings and ranks of the variables in a given subset. The research hypothesis was tested using Wilcoxon’s matched-pairs and Spearman’s rank correlation tests. The choice of these techniques was based on the recommendations of Cooper and Emory (1995), given that the test situation involves ordinal measurement scale and two-sample or bivariate analyses. The multiattribute analyses as used in this study was based on the multi-attribute utility approach of Chang and Ive (2002), and involved the estimation of the following parameters.
Mean rating (M): this was computed as the sum of the product of each rating point (P) and the corresponding percentage response to it (R%), out of the total number of responses (T) involved in the rating of the particular variable, i.e.
Pi5rating point i (1,i,5); Ri%5percentage response to rating point, i. The mean rating could be the importance index (II), the performance index (PI) or satisfaction index (SI). It seeks to evaluate respondents’ collective rating of a variable on the rating scale used.
Relativity index (RI): this was used to compare the M values of the variables in a given subset. It was computed as a unit of the sum of M’s in a subset of variables (equation4).
The relativity index could stand for the relative weight (Rw) or the relative importance index (RII) of a given attribute in a subset.
The satisfaction score (Ss): this represents the relative contribution of each subcomponent to the level of satisfaction derived from a major subcomponent of overall satisfaction. It was computed as the product of two parameters: the relative weight (Rw) of the subcomponent established in the development of multi-attribute model, and the satisfaction index (Si) of the subcomponent computed as the sum of products of rating points and their corresponding percentage responses (R%), i.e.:
The total satisfaction level (ST) derived from each major component, is therefore the sum of satisfaction scores (Ss) for all the subcomponents or attributes involved;
where N5number of subcomponents underlying the major component of overall client satisfaction in question.
Criticality index (CI)
To prioritize identified areas for improvement, the CI concept was used. The development of the concept draws from the understanding that the extent of the need for improvement in a given attribute of professional service is dependent upon the level of importance attached to it by clients, and the perceived level of satisfaction currently delivered by the professional group concerned (Nkado and Mbachu, 2002). Thus the criticality for improvement increases with the level of importance (as indicated by the importance index) and decreases with the satisfaction level currently delivered (as reflected by the performance index). This means that the need for improvement in a highpriority service attribute would not be critical if the professionals concerned are already delivering high levels of satisfaction. However, the reverse is the case if satisfaction level is perceived to be low. Consequently the CI is computed as:

CI5criticality index; II5importance index; PI5performance index.
The target population of clients was administered 223 copies of the questionnaire; 75 were returned, of which 64 were found usable. This implied a low 29% effective response rate.
Analyses of respondents’ demographic profiles showed that 34.4% were investors/developers, 18.8% were investors/owner-occupiers, while owner-occupiers comprised 7.8%. Developers (7.6%), investors (6.9%) and those who were equally exposed to property development, investment and owner-occupation (24.5%) constituted the rest of the client groupings. The views of the combined client groupings were predominantly those who engaged in both property development and investment.
In terms of organizational status, 53% of the clients were managing directors of their various organizations, and therefore made high-level inputs to the study. Other respondents occupied managerial (23%) and director/senior executive (17%) positions. The survey responses were from key decision makers who evaluate outcomes of the procurement process. This status of the respondents is expected to enhance the reliability and validity of the conclusions emanating from the findings of the study, though the reliability and validity of the findings were additionally verified through internal validation of the developed research models.
Re-scaling the Likert points from ordinal to interval scale
Before interval level analyses of the Likert scale data were carried out, the ordinal scale in which the responses were captured were converted into interval scale to achieve more meaningful interpretation of the results of the analysis. Nkado and Meyer (2001) point out that it is incorrect to assume linear or proportionate intervals between Likert rating points that are not rescaled. A correspondence analysis toolkit of the Number Cruncher Statistical Software (NCSS) was used to re-scale the Likert rating points from ordinal to interval scale for each set of ratings in the questionnaire, as documented by Bendixen and Sandler (1995).
Clients’ expectations and ratings of performances
Analyses of clients’ responses to levels of importance of their requirements, and performances of consultants and contractors in fulfilling them, are summarized in the following sections. Table1 shows an example of the analysis in respect of the quantity surveying services.
Clients’ expectations from quantity surveying services were explored during the pilot interviews. Levels of importance of the identified variables were rated during the quantitative surveys. Table1 presents clients’ responses, which are subjected to multi-attribute analysis for the purpose of establishing the mean ratings from clients’ collective points of view.
For ease of presentation, Table1 is split into two parts. The first part presents analysis of the relative importance of clients’ requirements from quantity surveying services, while the second part presents the relative performance of the group in meeting these requirements, as well as the perceived general satisfaction level delivered by the group. In Table1a and 1b, the rating scales for levels of importance are: VI5very important; JI5just important; SI5somewhat important; LI5of little importance; NI5not important. Similarly, levels of performance range from D (dissatisfactory) to VS (very satisfactory). In both tables, TR5total number of respondents; M5mean rating (equation1); RII5relative importance index (equation4); So5satisfaction score (equation1).
(a) Accurate and reliable cost and budget estimates, feasibility/viability and risk assessments.
(b) Service efficiency (timely job execution and comprehensiveness of cost information).
(c) Ability to foresee and budget reasonably for potential causes of cost escalations.
(d) Efficient performance of duties as per terms and conditions of appointment/engagement.
(e) Demonstration of competency (expertise and experience) for the job.
Table 1b	Quantity surveyors’ performances
(B) Quantity surveyors’ performance

Satisfaction level based on multi-attribute measures (SST)5
Satisfaction level based on single evaluative (overview) response (SSo):
Table1a presents clients’ requirements or expectations from quantity surveying services. Results of the multiattribute analysis show that clients accord topmost priority to accurate and reliable cost and budget estimates, feasibility and risk assessments (RII50.23).
On a re-scaled four-point rating continuum, multiattribute analysis shows that the level of client satisfaction delivered by quantity surveyors falls within the ‘just satisfied’ range (Sc52.28). However, overview response analysis shows that the perceived level of satisfaction falls within the ‘somewhat satisfied’ level (Sc51.97), which implies average satisfaction level. Here, Handy and Ptaff’s (1975) argument for multiitem analysis of satisfaction appears superior to Czepiel and Rosenberg’s (1977) preference for single evaluative measures, given that clients’ response to an overall satisfaction measure only crudely measures overall satisfaction.
Criticality index (CI) analysis shows that accurate and reliable cost and budget estimates, feasibility and risk assessments are the most critical areas for improvement, given the high level of importance and perceived low level of performance of quantity surveyors in meeting the need.
From the relative importance index values (RII) in Table1, the quantity surveying (SQ) subcomponent of equation3 is of the empirical form:
Quantity surveying consultants could therefore use this model to estimate the level of client satisfaction from their services if the perceived performance ratings (Sqi) are established from the client at any stage of the development process.
Client requirements from the services of other consultants and contractors
TablesA1–A4 in the Appendix summarize the results of similar analyses for other consultants and contractors. The tables present, for each group, prioritized clients requirements, satisfaction levels as analysed from both multi-attribute and single evaluative measures and the multi-attribute estimation model.
Relative contributions of subcomponents to satisfaction levels
Clients’ ratings of the relative contributions of satisfaction levels delivered by each group in contributing to the overall level of satisfaction in the development process are analysed in Table2. The purpose of the analysis was to determine empirically the relative weights of the subcomponents of the multi-attribute model for assessing client’s overall satisfaction in the development process in line with equation3.
Table2 shows that clients perceived the level of satisfaction with quantity surveying services as having the most profound effect on their overall level of satisfaction in the development process. In quantitative terms, this accounts for 29.5% of the total level of satisfaction expressed as a percentage.
On the basis of the relative weight (Rw) values of Table2, the constants in equation3 – A, B, C, D and E – corresponding to clients’ perceived levels of contributions of the group services in contributing to overall satisfaction in the development process have been empirically determined. Equation2 could therefore be re-written as:
Monitoring satisfaction levels in the development process
Built environment service providers do not have the luxury of making amends after a dissatisfactory outcome in a post-purchase satisfaction survey, as dissatisfied clients may not allow a ‘second chance’. Using the empirical models of equations8 and 9, the level of client’s satisfaction at any stage of the development process (preferably at pre-defined intervals through each stage) could be assessed by asking the client to rate performance on each relevant attribute of the operating component at the pertinent stage. Table3 presents a satisfaction level assessment model for each distinct stage of the development process in relation to the operating subcomponents (group services). The relative importance index (RII) values as empirically determined are adjusted to unity at each stage. The adjustment produces new relative weights (RIIadj) for evaluating the satisfaction level at any given stage.
Overall satisfaction levels
Overall satisfaction levels in the development process were analysed from the multi-attribute and overview response analyses in Table4. The multi-attribute approach was based on Equation2, with a summation of the product of the relative weight and the satisfaction score for each group of service. The overview response
Table 2	Contributions of subcomponents to client’s overall satisfaction
(a) Costing services;
*TR5total respondents; II5importance index (sum of products of % relative contribution and % responses for each of the nine % relative contribution categories for a given group service component); Adj II5adjusted importance index to ensure 100% total value for all Iis.
Conception	Costing services (SQ): 0.295	0.535	5	0.535[0.230Sq1+0.202Sq2
Architectural services	0.257	0.465	6	0.465[0.195Sa1+0.171Sa2
Design and procurement
Costing services (SQ):
Architectural services (SA):
Consulting engineering services (SE):
Construction
Costing services (SQ):
Architectural services (SA):
Consulting engineering services (SE):
Construction services (SC):
*RIIadj5adjusted RII; ai5relative weight of the ith attribute in a given set; Sqi, Sai, Sei, Smi, Sci,5client’s rating of the level of satisfaction derived from an attribute of client’s requirements or expectations from costing, architectural, consulting engineering, construction/project management and contracting services, respectively.
analysis was based on Equation1, involving a summation of the products of the rating point and percentage responses to give clients’ mean rating of the satisfaction level delivered by each group of service providers.
Results of both multi-attribute and overview response analyses showed average (‘somewhat satisfied’) levels of client satisfaction in the development process.
Internal validation of the research models
In addition to guiding the quest for data and achieving the research objective, a hypothesis was formulated with the aim of testing the internal reliability and validity of the developed models. The hypothesis tested whether the overall client satisfaction level estimated from the analysis of clients’ overview responses would differ significantly from similar estimates using multiattribute analysis.
As the test characteristics comprise ordinal measurement scale and two sample cases, the appropriate statistical methods are Wilcoxon’s matched-pairs and Spearman’s rank correlation tests (Cooper and Emory, 1995). Wilcoxon’s matched-pairs test was used to test for significance in differences between the set of satisfaction scores obtained from multi-attribute analysis of clients’ requirements from the services of the professionals and contractors, and the corresponding set obtained from the analysis of clients’ overview responses to levels of satisfaction with the services of consultants and contractors.
Spearman’s rank correlation coefficient was used to complement the reliability test by testing the
Table 4	Summary of satisfaction scores
Multi-attribute analyses	Overview response analyses
1	Costing services
6	Overall satisfaction levels
*Overall satisfaction rating on ordinal five-point rating scale:
(*Additional consistency check on overview responses) Correlation test:
Spearman’s rank correlation coefficient (Rho): multi-attribute versus over
view satisfaction scores:
*Si5satisfaction score (eqn3; Tables1, A1–A4); Ss5relative satisfaction score (eqn5); Rem5satisfaction level of the Si value on the re-scaled rating range: JS5just satisfied; SS5somewhat satisfied; Rel Wt5relative weight.
significance of the correlation between the satisfaction scores obtained from the multi-attribute analysis and analysis of clients’ overview responses. Parameters used in the test (i.e. multi-attribute and overview satisfaction scores) were analysed in Table1 and are summarized in Appendix TablesA1–A4. Table4 presents the test data which was inputted into the NCSS analysis template.
In the Wilcoxon’s matched-pairs test, the null hypothesis assumed that the differences between the matched pairs of satisfaction scores analysed from both multi-attribute and overview response analyses would be zero or non-significant. The alternative hypothesis was that the multi-attribute satisfaction scores would be significantly higher.
In the Wilcoxon’s matched pair, the null hypothesis was not rejected (Appendix, TableA5). The result implies that satisfaction levels analysed from multiattribute evaluations are not significantly different from those derived from overview responses. Perhaps this lends credence to Czepiel and Rosenberg’s (1977) argument that a summary view of satisfaction is valid.
In addition, the Spearman’s rank correlation test result showed a positive statistically significant correlation (coefficient of 0.77) between both sets of satisfaction scores. The correlation was confirmed by the result of a Kolmogorov–Smirnov test, which suggests that both sample distributions are similar.
The test results satisfy the internal consistency and equivalence perspectives on reliability (Zikmund, 1994; Cooper and Emory, 1995) as one measure of the satisfaction phenomenon under study (i.e. multi-attribute satisfaction estimates) correlated significantly with an equivalent measure (i.e. overview response estimates).
Practical application and limitation of the empirical models
The empirical models for measuring client needs during the development process (see Table3) are for generic applications in the popular traditional procurement process. The relative weights of the requirements of clients from the services of the consultants and contractors as shown in Table3 are generic and could vary over time, for individual clients, and for specific projects. However, a template for the assessment of individual client requirements for all types of procurement arrangement options and at any point in time is shown in TableA6 in the Appendix. The application of this template in a case study for assessing an individual client needs and satisfaction levels was reported elsewhere (Nkado and Mbachu, 2001).

This study sought to develop a conceptual framework for the assessment of client needs and satisfaction in the building development process. In addition, the developed framework was used to assess and prioritize clients’ needs and requirements from professional services. Through the criticality index (CI) analyses, areas for improvement in professional groups services were explored.
Results of the analyses of clients’ needs and requirements from the professions showed that, in quantity surveying services, clients place premium on accurate and reliable cost and budget estimates, feasibility and risk assessments. Criticality index analysis showed that this set of expectations is the most critical area for improvement given the high level of importance attached to it by clients, and the perceived low performance of quantity surveyors in this area.
For architectural services, clients attach highest level of importance to designs which are within their budgets, yet adequately address their main needs and requirements. This need was also found to be the most critical area for improvement in architectural services.
Safety and economy in design are the priority expectations of clients from engineering services. Criticality index analysis also confirms this to be the most critical area for improvement given the relatively low performance of engineers in this respect.
For both construction project management and contracting services, clients’ utmost expectation is to receive the completed building within agreed time, cost and quality targets. However, whereas this area of service is the most critical for improvement in the services of construction project managers, criticality index analysis showed that accommodating clients’ changes in good faith is the most critical area for improvement in the services of contractors.
Overall, both multi-attribute and overview response analyses showed that, on a four-point rating continuum, clients are only ‘somewhat’ satisfied with the services of key professional groups and contractors engaged in the development process.
A proactive and dynamic approach to client needs and satisfaction assessment was developed and validated in the study for use by consultants in the assessment of client needs, measurement and monitoring of client satisfaction levels at distinct phases of the development process. The established stage evaluation models are recommended to the project team to enhance client satisfaction levels in the building industry.
Acknowledgements
This research was supported by the RICS Education Trust, UK, to whom the authors are grateful.
