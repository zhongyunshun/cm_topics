Bond University
Research Repository
Skitmore, R. M.; Stradling, S. G.; Tuohy, A. P.
Published in:
Construction Management and Economics
Link to output in Bond University research repository.
Recommended citation(APA):
Skitmore, R. M., Stradling, S. G., & Tuohy, A. P. (1989). Project management under uncertainty. Construction
Management and Economics, 7(2), 103-113.
Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.
For more information, or if you believe that this document breaches copyright, please contact the Bond University research repository coordinator.
Download date: 28 Mar 2023
R.M. Skitmore, S.G. Stradling and A.P. Tuohy
Department of Civil Engineering, University of Salford, Salford M5 4 WT, UK
Morris' (1986) analysis of the factors affecting project success and failure is considered in relation to the psychology of judgement under uncertainty. A model is proposed whereby project managers may identify the specific circumstances in which human decision-making is prone to systematic error, and hence may apply a number of de-biasing techniques. Project management, construction, uncertainty
Introduction
The influence of uncertainty in the construction industry, and of the risks generated by such uncertainty, has been of increasing concern over the two decades since the report of the Tavistock Institute (1966). The awareness that uncertainty does indeed lie at the heart of many of the industry's organizational problems suggests that research in this direction is a priority, and that any knowledge that may help decision-makers in the construction industry to recognize and minimize uncertainty and risk is likely to be of some potential value.
Morris (1986) surveyed a heterogeneous sample of large projects and provided ample evidence of the influence of uncertainty in such contexts. Our analysis of his results identifies a list of 'lessons', embodying important principles which he considered to bear on the success or failure of these projects. Perhaps as one consequence of the breadth of his study's frame of reference, however, the 'lessons' themselves are rather heterogeneous (see Table I), and the imposition of a more direct, and perhaps more theory driven, structure on this wide-ranging array of information will aid the explication, understanding and application of strategies for maximizing its broader import.
One clear implication of Morris' work is that the lessons convey information which may be used to define decision making strategies whereby the impact of uncertainty is minimized. Such strategies are not likely to consist of sequences of rigorously logical inference, however; human judgement is a more complex and idiosyncratic phenomenon, with an internal logic of its own. Nevertheless, it is precisely because of these idiosyncratic operating characteristics that human judgement is very often a source of uncertainty rather than a means of dealing with it, and it may therefore be the case that the systematic processes which underlie human judgement may provide an appropriate paradigm within which to organize the information contained in Morris' lessons.
The psychology of judgement
It has been well established that human judgement (i.e. predictions or evaluations based on incomplete or uncertain information) generally is subject to systematic error, or bias, as well as to unsystematic error, or variance (Bowman, 1963). In the evolutionary context, such judgemental biases were adaptive, providing cognitive shortcuts and tending to optimize a trade-off between psychological processing demands and the need for response accuracy. To a great extent this remains the case, and these intuitive decision mechanisms enable us to function on a day to day basis without having to analyse enormous amounts of information. The complexity of socio-technical systems, however, have created many decision situations in which accuracy is essential and in which 'wired-in' cognitive biases may be maladaptive.
A considerable psychological literature has developed from the study of judgemental error and the processes of decision under uncertainty. Hogarth (1980) reviewed this research area and organized the more important findings into a framework which broadly corresponds to the major sequential stages of cognitive activity, namely the acquisition of information, analytical processes, output mechanisms, and the assessment and evaluation of feedback. At each of these four important stages, a number of psychological biases have been observed which in general terms help us to deal with the multiplexity of the world, but which in specific situations, especially those calling for high levels of accuracy, can be highly misleading.
biases in the acquisition of information
As suggested by Hogarth (1980), the key issue in determining bias at this stage is the salience or 'strikingness' (Berlyne, 1971) of information, either as it is perceived or as it is retrieved from memory. Thus, instances which are more readily recalled are likely to be judged as more important or more frequent than those which are less easily accessed (the availability bias) (Tversky and Kahneman, 1973). Various forms of selective perception may also be operative: thus, people perceive problems in terms of their own experience; they see what they expect to see; they tend to seek confirmatory evidence, and to disregard or simply fail to perceive evidence which conflicts with their pre-assumptions (Wason, 1960). These examples of bias are primarily derived from the characteristics of the observer.
Other potential sources of bias are at least partially based on aspects of the information. Information which is concrete is very much more salient than abstract information, and hence a single experiential incident may be given spurious weight in the decision process while more representative but less vivid data are disregarded (Borgida and Nisbett, 1977). Frequency biases may also be operative, in which people focus on the number of times an event has occurred and ignore the number of times when it might have occurred but did not. Finally, the mode of data presentation can also affect salience, by means of context effects, order effects, etc.
biases in the processing of information
Potential sources of bias at this stage are derived from the mental processes applied to information once it has been perceived or retrieved from memory. As before, some of these may be seen to derive from aspects of the task itself, and from the conditions under which the decision is made. The general consistency of the information sources in decision making can lead to judgement becoming more confident, but not necessarily more accurate. Stress, introduced by social situations, group dynamics, emotion, time pressure or information complexity, can lead to serious distortions of judgement (Wright, 1974). For instance, it is more difficult to make accurate decisions under conditions of annoyance or distraction, or under the scrutiny of others.
Other sources of bias at the processing stage derive from the operating characteristics of the human cognitive system itself. The conservativism of the system is likely to be expressed in terms of the failure to revise an existing judgement on receipt of new information (Edwards, 1968). One well documented context in which judgement is typically very conservative is the selection interview. Research has shown that interviewers often seek information to confirm their first impressions, and are reluctant to revise their impressions in the face of conflicting information (Arvey and Campion, 1984). The inconsistency of the system (Bowman, 1963) is likely to be reflected in an inability to apply a consistent judgemental strategy over a series of similar tasks. Perfect predictability would of course be an evolutionary handicap, and inconsistency or surprise may be an advantageous strategy in dealing with one's competitors, but in situations requiring accurate judgements on a repetitive basis this is not the case. Interviewers, for example, are prone to allowing a succession of good or bad candidates to shift their standard of judgement (Wexley et al., 1972).
The characteristics of the human information processing system are also seen in the application of a number of cognitive heuristics, i.e. systematic rules or principles which operate instead of a detailed analysis of the available information, thus conserving mental effort. Heuristics, and the biases to which they give rise, have been the subject of a great deal of research, to the extent that Berkeley and Humphreys (1982) have claimed that there is now a 'bias heuristic' in the psychological literature itself, i.e. a tendency to identify any observed bias as a new cognitive heuristic.
In spite of this criticism, a number of heuristic biases are well established. Habit may be a heuristic, as when an alternative is chosen simply because it was previously satisfactory. The representativeness heuristic (Kahneman and Tversky, 1972) judges the likelihood of a particular event by estimating how typical it is of the class of such events; thus the perceived probability that an individual is, say, an engineer rather than a lawyer is increased by the provision of information about his personality which is actually irrelevant, but which fits the respondent's idea of what an engineer should be like. The anchoring and adjustment heuristic (Tversky and Kahneman, 1974) comes into play when predictions are made by extrapolating, typically insufficiently, from an initial value. Lichenstein et al. (1978) asked subjects to estimate the yearly frequency of death from various causes, but primed the subjects by telling them either that 50 000 people died in car accidents or that 1000 people died from electrocution. The latter group, who were anchored on the smaller number, reliably returned smaller estimates.
Other cognitive heuristics also derive from the inability to handle probabilistic information. Belief in the law of small numbers (Kahneman and Tversky, 1971) is the assumption that the characteristics of small samples are representative of those of the population. One example of this is seen in the common tendency to assume that the opinions, attitudes and values of ourselves and those around us are more generally held in the population at large than is actually the case (Manstead, 1982). Regression biases result when extreme outcomes give rise to extreme predictions, thus neglecting the phenomenon of regression to the mean. The best-guess strategy (Gettys et a/., 1973) is operative when there are several sources of uncertainty; these are resolved by treating the most likely outcome as certain in each case. Thus, the fact that information is uncertain may be ignored altogether.
biases in output processes
Some output biases can result from the format of the required judgement, e.g. response mode (the way in which a judgement must be expressed can affect its outcome) or scale effects (the available scale of response values can influence judgement). Other possible biases at this stage can more directly be attributed to characteristics of the judge, as in wishful thinking, where preferred outcomes are assessed as ipso facto more likely, or in the illusion of control (Langer, 1975), in which general activity concerning the outcome of an uncertain event creates the feeling that it can be controlled, even though this is not the case.
biases in feedback processes
As Hogarth pointed out, the interpretation and assessment of feedback is one of the most important aspects of judgement, since without it we cannot learn from experience. Accordingly, biases in feedback have important consequences for future judgements. As in previous stages, some of these biases are rooted in the inability to understand probability (e.g.
as in the gambler's fallacy whereby people tend to believe that a succession of heads increases the future likelihood of tails).
Other potential biases are based on attributional processes whereby explanations for success or failure are determined. In assessing other people's actions, judges are prone to what Ross (1977) termed the fundamental attribution error, i.e, the tendency to underestimate the effects of chance or situational factors in deciding the outcome, and to overestimate the effects of people's intentions and actions. In assessing one's own performance, there is a marked tendency to attribute success to skill, but failure to the effects of chance.
Similar attempts to assess past events often involve the tendency to reconstruct them. Recall fallacies are created by the imposition of a logical structure on imperfectly remembered events (Snyder and Uranowitz, 1978), and the well known unreliability of eye witness testimony is a vivid example of this. In hindsight, too, people are often not surprised by events. They can readily explain why an outcome was predictable, and they are likely to think that they expected it all along (Fischhoff and Beyth, 1975).
Enabling conditions for biases
Hogarth's (1980) extensive typology of particular biases was augmented by a useful discussion of the conditions under which bias in general is likely to occur (see Table 2). As noted above, many judgemental biases can be primarily attributed either to characteristics of the task itself, or to those of the schema, i.e. the strategies, heuristics, assumptions, attitudes, etc. of the judge. Of course, in many instances these must operate interactively. Nevertheless, at the theoretical level a valid distinction can be made between task generated and schema generated conditions for bias.
The three major task generated conditions for bias distinguished by Hogarth are as follows. High potential for bias occurs when the decision task has a high degree of complexity; when it has a high degree of procedural uncertainty; and when it is performed under circumstances involving a high degree of stress. In addition to these, a fourth factor called psychological regret provides an index of the extent to which error in a decision task affects the decision maker's feelings; for example, whether there is personal responsibility for a negative outcome, or directly negative consequences for the decision maker. Hogarth argued that high potential for bias occurs with low psychological regret, i.e. if the task is so organized as to leave the decision maker unaffected by the outcome he is more likely to base his decisions on fast, psychologically economical, highly approximate heuristics, rather than on laborious, psychologically costly, accurate analysis (Jacob et al., 1986).
The three major schema generated conditions for bias are all to do with the way in which a person approaches a decision task, rather than with the objective factors which exist in the task itself. They are the veridicality of the schema, i.e. the extent to which a person's understanding of the task is accurate and his approach is realistic; its stability, i.e. the extent to which related decisions are approached in a consistent way; and its generality, i.e. its applicability to a wide range of contributary decision tasks.
Hogarth suggested that high potential for bias is derived from the use of schemas with low veridicality, stability, or generality. Hogarth presented this typology in terms of a static array of circumstances, which of themselves determine the likelihood of bias in human judgement. Nevertheless, it is evident that an internal dynamic exists, whereby the current state of one element may affect others, and Hogarth noted that, for instance, potential for bias under high schema generality depends on veridicality. In other words, a decision maker who has an inaccurate concept of a project's priorities (low schema veridicality) is likely to introduce a high degree of bias if this non-veridical schema informs a wide range of project decisions (high schema generality). A possible version of the structure underlying this contingency is shown in Fig. 1.
Enabling conditions for biases in Morris' lessons
We believe that Hogarth's organization of these findings in the psychology of judgement offers a convenient framework within which the need to recognize and reduce uncertainty in large projects can be accommodated. Indeed, a systematic examination of the
correspondences between Morris' lessons and Hogarth's enabling conditions for bias suggests that the lessons can be categorized in terms of their primary context of bias potential (although of course some of the more general examples may extend to more than one such context). This re-organization is set out in Table 3.
An inspection of Table 3 suggests that the application of a theoretical typology of bias inducing circumstances is an effective way of organizing and reducing the multifariate array of lessons to a smaller number of general principles. A primary bias reduction strategy clearly lies in a policy of simplification. To this end, projects should be segmented (lesson 2), design specifications should be straightforward (lessons 11 and 13), the legal, financial, and organizational arrangements should not be over-complex (lessons 26, 27 and 28), and the bidding arrangements should not be over-competitive (lesson 32). Thus, by implementing this group of lessons, a project manager is ipso facto minimizing the enabling condition for judgemental bias which arise out of task complexity.
In a similar way, for instance by avoiding the need for late design changes (lesson 12), procedural uncertainty can be minimized, together with the propensity for specific biases which may arise out of it. If, for example, the circumstances within which procedural uncertainty occurs are to do with the acquisition of information, then selective perception or the availability bias may be the specific biases to which judgement is most liable, and the ' effects of these can be reduced and their likelihood minimized by complying with the lessons which can be considered relevant to this task characteristic. In this way, the whole of Table 3 can be read as a prescriptive for the reduction of judgemental bias in specific circumstances.
An inspection of Fig. 1 suggests that a special place in the dynamics of this model is occupied by schema veridicality, i.e. by the extent to which the decision maker's concept of the project corresponds to the real, objective situation, and by psychological regret, i.e. the cost to the decision maker of making an erroneous choice. This emphasis on participants' attitudes, objectives, task and social orientation, motivation etc., shows some correspondence with the contingency management approach. The overall organization must therefore be carefully designed to be appropriate to the project, with full consideration of the participants' contribution to decision making, and provision for feedback and consultation at all stages. It is also a feature of contingency management that schema generality is maintained, with flexible philosophies applied to all aspects of the project. As shown in Fig. 1, the value of an effectively general schema is likely to depend on its veridicality.
It is of interest to note that Hogarth's broad analysis of human judgement under uncertainty has recently been tabled by Jacob, Gaultney and Salvendy as an agenda for the construction and use of expert systems (1986). Jacob et al. were primarily concerned to contrast the operational bases of human and expert system decision making, and they pointed out that expert systems do not explicitly consider risk as a factor in their analysis. Humans, by contrast, 'use rapid, direct decision making processes when perceived risk is low and reserve laborious, analytical decision making approaches for high risk situation' (p. 139), and Jacob et al. singled out the accurate perception of risk as a basis for a greater similarity between human and computer decisions. This suggestion points to the central axis of Fig. 1, where psychological regret interacts with veridicality, and supports the notion that these factors are particularly important in the implementation of judgemental bias.
The analysis of project management under uncertainty suggests that the model may be useful at two major levels. The first of these is the use of Morris' lessons to identify which, if any, of Hogarth's enabling conditions are likely to comprise potential sources of judgemental bias in any particular project. If, for example, it is impossible to avoid time pressures (cf. lessons 19 and 30), then planners should be aware that such constraints on the decision environment may lead to stress, and to bias in the decision making process. The second level is to identify at what stage of the information processing sequence such bias is likely to be operative, and hence to determine what likely forms it may take. If, in the same example, the pressures of time are operative on the acquisition of information then the decision maker may be liable to err in the direction of more salient items (see above). The knowledge that human judgement is prone to particular biases under particular circumstances is of course the key to overcoming them.
Fischhoff (1982) classified debiasing methods according to the source of the bias, i.e. task, judge, or task/judge mismatch, but a prescriptive index of bias elimination techniques appropriate for all circumstances and types of project would be both cumbersome and (given the probabilistic nature of psychological biases) rather inefficient. Nevertheless, since we have argued that the dynamics set out in Fig. 1 point to a central role for psychological regret and schema veridicality, some general corrective methods appropriate to these categories may be identified. In any event, it is likely that these are the aspects of a project most readily adjusted, once its parameters have been set.
In the context of veridicality, it is clearly of primary importance that all major participants in a project are working with the same goals and priorities, and that these are not at variance with the realities of the situation. The main concern here is therefore communication, and relevant debiasing strategies laid out by Fischhoff include: clarifying of instructions; warning about problems; making knowledge explicit; and eliminating discrepant information. In terms of the dynamics set out in Fig. 1, it is important to note that effective debiasing at this level is likely to have concomitant effects on both the generality and the stability of the prevalent schema.
In the context of psychological regret, the main operative factors are the allocation of responsibility, and the extent to which that is felt to be equitably distributed among the participants. Since bias may be associated with low levels of psychological regret, one appropriate method of debiasing an individual decision maker is to raise the stakes for that individual. This is, of course, more constructively achieved by increasing job commitment and thereby raising the general morale of the workforce than by passing the adverse consequences of a poor decision down the line. The relationship between psychological regret and stress (Fig. 1) suggests that an optimal balance should be sought between increasing an individual's interest in the outcome of his decisions and imposing a stressful level of responsibility on him, which would tend to re-introduce bias from a new direction.
When no further corrective procedures in terms of Hogarth's system are available, the manager has three main alternative courses open to him in the face of biased decision making. He can replace the individual; he can ignore or accept the error; or he can recalibrate the decision by applying an adjustment. This latter course clearly pre-supposes a knowledge of the direction and extent of the error, and one of the purposes of this paper is to provide some rudimentary tools that will assist the project manager in that endeavour, enabling him to optimize the decisions which he and his team must make.
Acknowledgements
The work reported here was supported by a grant from the Science and Engineering Research Council specially promoted programme in construction management (GR/D/55856) to the first two authors.
