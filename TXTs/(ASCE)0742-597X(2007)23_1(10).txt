Management of Construction Firm PerformanceUsing BenchmarkingMohammad S. El-Mashaleh1; R. Edward Minchin Jr.2; and William J. O’Brien3Abstract: In today’s highly competitive construction industry, there is a critical need for managers to continuously improve their firm’s efficiency and effectiveness. More specifically, managers need to know which performance measures are most critical in determining their firm’s overall success. Benchmarking, when done properly, can accurately identify both successful companies and the underlying reasons for their success. However, rigorous benchmarking within the construction industry still remains an embryonic field. This paper analyzes and critiques both the performance measures and metrics used traditionally in the construction industry and the benchmarking models developed to date for the industry. Based on the results of this analysis, benchmarking models using data envelope analysis are proposed that offer significant improvements over current models. The proposed models measure construction firm performance on a company-wide basis, foster trade-off analyses among various performance metrics, and tie the resources expended by construction firms to how well those firms perform overall. The models also provide managers guidance in determining how specific company resources can be reallocated to improve overall company performance. DOI: 10.1061/ASCE0742-597X200723:110CE Database subject headings: Bench marks; Construction industry; Contractors; Performance characteristics; Research.
IntroductionLeading a construction firm is a challenging task in today’s highly competitive industry environment. The industry is volatile and, until fairly recently, was not overly sophisticated in terms of leadership and management, nor was it aggressive in identifying industry-leading practices. In recent years, however, the industry has become more aware of its need to identify, implement, and sustain performance improvements more systematically. In one example of this recognition, benchmarking has become more commonly discussed as a tool that can be used to identify successful construction companies and the reasons for their success.  Benchmarking aims at comparing the performance of firms relative to each other, allowing these firms to recognize their weaknesses and strengths compared to the industry. Benchmarking aids in the identification of industry leaders who exhibit superior performance as a result of using best industry practices. By finding examples of superior performance, firms can adjust their policies and practices to improve their own performance and become more similar to performance leaders in the industry.1    Assistant Professor, Dept. of Civil Engineering, Hashemite Univ., Jordan, P.O. Box 150459 Zarqa 13115 Jordan. E-mail: mashaleh@ hu.edu.jo2    Assistant Professor, Dept. of Civil and Coastal Engineering, Univ. of Florida, 365 Weil Hall, P.O. Box 116580, Gainesville, FL 32611-6580. E-mail: minch@ufl.edu3    Assistant Professor, Dept. of Civil Engineering, Univ. of Texas at Austin, 1 University Station C-1752, Austin, TX 78712. E-mail: wjob@mail.utexas.edu   Note. Discussion open until June 1, 2007. Separate discussions must be submitted for individual papers. To extend the closing date by one month, a written request must be filed with the ASCE Managing Editor. The manuscript for this paper was submitted for review and possible publication on July 13, 2005; approved on May 1, 2006. This paper is part of the Journal of Management in Engineering, Vol. 23, No. 1, January 1, 2007. ©ASCE, ISSN 0742-597X/2007/1-10–17/$25.00.   In the long term, success of both individual construction firms and the industry overall will depend on improving performance by continually acquiring and applying new knowledge. This will require a more comprehensive understanding of how existing practices can get better. Construction firms therefore need benchmarking tools that provide perspective on both their current practices and existing shortcomings versus industry leaders. Existing benchmarking models have limitations in their ability to guide the industry toward more efficient and effective performance. Thus the industry needs new benchmarking models that offer managers clear insights into both current practices and pathways toward improving future performance.   This paper proposes such models. These models are robust and overcome the limitations of the models developed to date. The proposed models provide insight into overall firm performance and support trade-off analyses among multiple key performance metrics. Additionally, the proposed models allow for evaluating the resources expended by a firm against the overall level of success the firm achieves.   The proposed benchmarking models were developed based on actual field data collected from 74 construction firms. These firms included general contractors, construction management companies, design/build firms, and subcontractors. The companies are involved in residential, commercial, industrial, and heavy/ highway construction. Fifty percent of the firms have revenues in excess of $50 million.Literature ReviewCamp 1989 defined benchmarking as, “The continuous process of measuring products, services, and practices against the toughest competitors or those companies recognized as industry leaders.” The Construction Industry Institute CII has adopted the following definition of benchmarking: “A systematic process of measuring one’s performance against results from recognized leaders for the purpose of determining best practices that lead to superior performance when adapted and implemented” Hudson 1997.   The literature review consisted both of identifying previously developed construction industry key performance measures and metrics and reviewing the main industry benchmarking models developed to date. Once the literature summaries of key industry metrics are reviewed below, the three most widely used construction industry benchmarking models—those developed by Fisher et al. 1995, Hudson 1997, and CII 2000, and the Construction Best Practice Program CBPP 1998 benchmarking model— are each reviewed and critiqued.Evolution of Construction Industry Performance Measures and Key MetricsCamp 1989 and Spendolini 1992 stated that identifying what is to be benchmarked, or the benchmarking metrics, is often one of the most difficult steps in the process. Hudson 1997 indicated that the CII adopted the following definition for a metric: “a quantifiable, simple, and understandable measure which can be used to optimize performance.” Hudson 1997 also indicated that the CII adheres to the following principles for metrics used in their benchmarking system:• A metric must provide a value to its stakeholders;• A metric must focus on continuous improvement and establish an objective target; and• A metric can be influenced by adoption of better practices.   Additionally, Hudson 1997 recommends utilizing The Metrics Handbook for devising metrics. The Metrics Handbook is published by the United States Air Force 1995; it characterizes a good metric as one that conforms to the following attributes: • It is meaningful in terms of customer requirements;• It tells how well organizational goals and objectives are being met through processes and tasks;• It is simple, understandable, logical, and repeatable;• It shows a trend, i.e., measures over time;• It is unambiguously defined; •	Its data are economical to collect; and •	It is timely.   Camp 1989 stated that benchmarking metrics are determined from the basic mission of the organization or business unit. Spendolini 1992 supports Camp’s statement by linking what is to be benchmarked to the critical success factors CSF of a business. CSF are those factors that have the greatest impact on the performance of the organization. Watson 1992 defines CSF as “The limited number of areas in which results, if they are satisfactory, will ensure successful competitive performance for the organization.”   Sanvido et al. 1992 adopt the Boynton and Zmund 1984 definition of CSF as, “Those few things that must go well to ensure success for a manager or organization, and therefore, they represent those managerial or enterprise areas that must be given special and continual attention to bring about high performance. Critical success factors include issues vital to an organization’s current operating activities and its future success.” In construction terms, Sanvido et al. 1992 defined CSF as, “Those factors predicting success on projects.”   To guide the selection of the benchmarking metrics, Camp 1989 and Spendolini recommend posing Xerox’s ten questions:• What is the most critical factor to business success e.g., customer satisfaction, expense to revenue ratio, return on asset performance?• What factors are causing the most trouble e.g., not performing to expectations?• What products or services are provided to customers?• What factors account for customer satisfaction?• What specific problems operational have been identified in the organization?• Where	are	the	competitive	pressures	being	felt	in	the organization?• What	are	the	major	costs or	cost	“drivers” in	the organization?• Which functions represent the highest percentage of cost?• Which functions have the greatest room for improvement?• Which functions have the greatest effect or potential for differentiating the organization from competitors in the market place?   McGeorge and Palmer 1997 address many of these questions for the construction industry. Key issues they identified for the construction industry include subcontractor management and meeting completion dates. They also identified some of the key factors responsible for customer satisfaction: completion on time; minimum and/or guaranteed cost; quality; and health and safety. These findings confirm findings uncovered elsewhere during the extensive literature search, and helped establish a consensus on the most important performance metrics for the construction industry. The review confirmed that schedule adherence, cost performance, customer satisfaction, safety performance, and profit are the performance metrics most critical to overall success in the construction industry. Therefore, the research team decided to proceed with the development of the benchmarking models using these five metrics to measure the company-wide efficiency of a firm. The development of the models is detailed later in this paper.Existing Benchmarking ModelsThe following discussion presents three existing construction benchmarking models:• Fisher et al. 1995; •	Hudson 1997 and CII 2000; and• CBPP 1998.Fisher et al. Benchmarking ModelThe Fisher et al. 1995 benchmarking model was the first prominent benchmarking model applied to the construction industry. The Houston Business Roundtable HBR assembled a group of owners and contractors to solicit ideas and compile initial benchmark data for use by the construction industry. Fisher stated that the HBR was motivated by the fact that “there are currently no available benchmarked standards for the construction industry, nor is there a nonprofit organization established for the purpose of collecting data and information.”   The study utilized a survey to collect data from 17 companies on 567 projects. Data were collected for actual versus authorized cost, actual versus target schedule, actual versus estimated construction labor, and change orders versus original authorized cost.Hudson and Construction Industry InstituteBenchmarking ModelHudson 1997 performed his benchmarking study under the guidance of the Benchmarking and Metrics Committee BM&M of CII. In 2000, the BM&M committee published a report thatTable 1. Metrics Industry Norms All Types of ProjectsMetric/formulaNorms meanProject budget factor0.974Actual total project cost/initial predicted project cost +approved changesProject cost growth0.075actual total project cost—initial predicted project cost/initial predicted project costProject schedule factor0.968Actual total project duration/initial predicted project duration+approved changesProject schedule growth0.027actual total project duration–initial predicted project duration/initial predicted project durationRecordable incident rate2.874total number of recordable cases  200,000/total site work hoursRecordable cases: All work-related deaths and illnesses and those work related injuries which result in loss of consciousness, restriction of work or motion, transfer to another job, or require medical treatment beyond first aid Lost workday case incident rate0.149total number of lost workday cases  200,000/total site work hoursLost workday cases: Cases which involve days away from work or days of restricted work activity or bothChange cost factor0.089Total cost of changes/actual total project costTotal field rework factorTotal direct cost of field rework/actual construction phase cost0.026showed the industry norms for some of the metrics developed by Hudson. The database for their report consists of 901 projects from 37 owner and 30 contractor companies. The report characterized projects based on the type of construction i.e., heavy industrial, light industrial, building, infrastructure, project size i.e., less than $15 million, between $15 and $50 million, between $50 and $100 million, over $100 million, project nature i.e., adds-on, grass roots, modernization, and project location i.e., global, domestic. The key performance measures and the associated metrics produced by this report are shown in Table 1.Construction Best Practice Program Benchmarking Model The CBPP benchmarking model is also known as the key performance indicators KPI model. The KPI model was developed and implemented in the U.K. construction industry and benchmarks a project or a company against the range of performance levels currently being achieved across the industry. It provides a framework to check how a construction company compares with the rest of the industry, and helps firms to focus on their main priority areas of improvement.  The CBPP report 1998 stated that construction industry clients want their projects delivered on time, on budget, safely, efficiently, free from defects, and by profitable companies. The ten KPI used in the model reflect the aforementioned criteria, as seven of the indicators relate to project performance, while the rest relate to company performance. Further explanation of this model is available in the literature CBPP 1998.Critique of Existing ModelsThe benchmarking models proposed previously for the construction industry all have significant shortcomings if the goal is a company-wide analysis. El-Mashaleh 2003 critiqued these benchmarking models and argued that, if the goal was to measure company-wide performance, they all fall short in four respects:1. Existing benchmarking models are project specific. They report project-level industry norms of some performance metrics i.e., cost, schedule, safety, etc.. This limited view communicates only a single metric performance for a specific project. No insight is provided into the overall performance of the firm;2. Current benchmarking models do not support an understanding of the trade-offs among the different variables that affect performance;3. The models provide no insight into the relationship between how resources are expended and the relative success of outcomes. Thus, there is no ability to determine the return on investment associated with specific firm actions; and4. As a consequence of being project specific, existing benchmarking models do not allow for the measurement of the impact of certain technological and managerial factors on overall firm performance. This makes it difficult to identify practices that lead to superior overall firm performance over the long term.Measuring Project-Level Industry Norms of Some Performance MetricsMeasuring project-level performance for only a few even wellchosen metrics does not translate into robust evaluation of an entire firm. Current models do not answer the question: Where does a certain firm stand compared to the other firms when considering overall performance i.e., all metrics simultaneously? An overall performance report card for each firm is needed. Rouse et al. 1997 declared that single measures of performance capture only a limited perspective of an organization’s activities. Several measures must be used in combination to gauge organizational performance. Overall firm performance evaluations take on particular importance when they can help prevent managers from improving one metric at the expense of hurting overall firm efficiency. Towill 2001 stressed that, “It is important to emphasize that improvement in one business performance metric say, cost must not be sought at the expense of another say, quality or safety.” This leads to the second shortcoming of existing models.Trade-Offs among Different Variables That Affect PerformanceThe current benchmarking models do not support an understanding of the trade-offs among the different metrics or resource requirements associated with overall firm performance. For example, a firm’s cost performance may improve, but schedule performance declines. How can one determine whether this trade-off is truly desirable? Is the overall performance of the firm better or worse? McKim et al. 2000 mentioned this trade-off when they stated that “as various techniques are available to control the cost, schedule, and quality individually, these three indicators of performance are highly interrelated and affect one another...these indicators are highly interrelated and require some balance and trade-off among them to achieve efficient overall control over project performance.”   Both Watkinson 1992a and Janssens 1992 list these tradeoffs among the metrics of performance. Watkinson 1992b casts some doubt, saying, “Any combination of these criteria time, cost, and quality can be achieved, but rarely all three.” Janssens 1992 agrees, saying, “There will always be a trade-off between time, cost, and quality as the client tries to balance thesevariables.”   This trade-off analysis takes on particular importance when plotting overall firm performance over time. Under the current models, only increases or decreases in one metric at a time are calculated, so these models cannot guide managers as they seek to understand the key survival question: Is the firm getting better over time? Performance over time also helps managers isolate and detect explanatory reasons for good performance i.e., track the implications of certain managerial and technological implementations.Cost/Performance RelationshipsCurrent industry benchmarking models do not examine the relationship between resource allocation how and what types of resources are expended—either on a specific project or across an entire company and how effectively and productively those resources are actually deployed. Two firms that arrive at the same performance level for a specific metric for example, revenues are considered to be similarly efficient. This is clearly not the case if one firm is expending more resources i.e., money, personnel, etc. than the other firm. It is clear that the firm that commits fewer resources to arrive at a certain performance all other things being equal is a better performer. Yet, no current industry benchmarking models incorporate this logic.No Measurement of Impact of Technological and Managerial Attributes on Firm PerformanceAs a consequence of being project specific, the existing benchmarking models do not allow measurement of certain technological and managerial attributes on overall firm performance. The writers pioneered a firm-level study in the area of information technology IT use and performance in construction ElMashaleh et al. 2006. This study addresses the difficulties in the measurement of certain technological and managerial attributes on overall firm performance and reports on the utility and challenges of advancing benchmarking capabilities in construction research. Those particularly interested in this aspect of firm performance can check the literature, as the focus of this paper will henceforth be on the other three areas of deficiency, described above.Benchmarking Model DevelopmentGiven the clear shortcomings of existing benchmarking models, a new, more comprehensive model was called for. The following section describes such models. The models use the five metrics of performance determined to be the most critical indicators of industry success based on the consensus of the construction literature and incorporate data envelopment analysis DEA, a tool that has proven highly effective in other data analysis applications and is described more fully later in this section.Metrics Used in Proposed Benchmarking ModelsThe proposed benchmarking models use both input metrics and output metrics to determine company performance. The two input metrics used are safety expenses and project management expenses. The metric for safety expenses is defined as the combined annual cost of safety programs and salaries of safety personnel as a percentage of total company sales. Firms that spend more on safety should be expected to have better safety performance. The metric for project management is defined as project management personnel salaries, as well as the annual costs of both project management training and project management software acquisitions and updates. Firms that spend more on project management should be expected to achieve better adherence to the other four metrics—schedules, cost control, customer satisfaction, and profit.   Five output variables are also used in the benchmarking models developed. Schedule performance is measured by how often projects are delivered on/ahead of schedule. Cost performance is measured by how often projects are delivered on/under budget. For both these metrics, data are limited to projects closed in the last two fiscal years. Customer satisfaction is measured in terms of the percentage of repeat business customers. Net profit after tax as a percentage of total sales for the last fiscal year is used to measure the profitability of the firm.   Safety performance is based on two indicators: experience modification rating EMR and occupational safety and health administration OSHA recordable incidence rate. These indicators were chosen based on an analysis done by the Business Roundtable 1982 that indicated that these metrics were “relatively objective measures of past safety performance.” EMR is a measure of workers’ compensation insurance premiums, while OSHA recordable injury and illness incidence rates are a widely used indicator of on-the-job safety. Levitt and Samelson 1987 and Jaselskis et al. 1996 have stated that both EMR and OSHA recordable incidence rates are useful in evaluating company safety performance over a number of years. The reciprocal of the EMR and the OSHA incidence rate are used in the proposed benchmarking models in order to convert these unfavorable measures to favorable ones the lower the values of EMR and OSHA incidence rate, the better the safety performance.   Each of the five output variables described above is defined more completely as to their use in the model in Table 2.Data Envelopment AnalysisDEA was initiated by Charnes 1978, and Rhodes 1981 based on the work of Farrell. Farrell 1957 proposed the notion of the structural efficiency of an industry. Structural efficiency is essentially a measure of the extent to which an industry tracks the performance of its own most efficient firms. It enables firms to assess their relative efficiencies compared to other firms in the industry.   DEA is concerned with evaluation of performance and it is especially concerned with evaluating the activities of organizations such as business firms, hospitals, government agencies, etc. In DEA, the organization under study is called a DMU decision making unit. A DMU is regarded as the entity responsible for converting inputs i.e., resources, personnel, money, etc. into outputs i.e., sales, profits, customer satisfaction, metrics of performance, etc.. In this paper, the DMU of interest is the construction firm as an integrated entity. It is their performance that was evaluated.Table 2. Metrics of Performance along with Their Measurement MethodMetricMeasurement/calculationSchedule performancePercentage of projects are delivered on/ahead of schedule in the last two fiscal years i.e., how often are projects delivered on/ahead of schedule?Calculation of metric used in model for projects closed in the last two fiscal years:percentagenumber of projects delivered on/ahead of schedule/total number of projects  100%Cost performancePercentage of the time projects are delivered on/under budget in the last two fiscal years i.e., how often are projects delivered on/under budget?Calculation for projects closed in the last two fiscal years: percentagenumber of projects delivered on/under budget/ total number of projects  100%Safety performanceOSHA recordable incidence rateCalculation of actual metric used in benchmarking model: reciprocal of last reported OSHA recordable incidence rateExperience modification rating EMRCalculation of actual metric used in benchmarking model: reciprocal of last reported EMRCustomersatisfactionPercentage of repeat business customersMetric used in model: percentage of customers who come back for a repeat business with the firmProfitNet profit after tax as a percentage of total sales for the last fiscal yearCalculation of metric used in model for the last fiscal year: net profit after tax/total sales  100%   DEA utilizes mathematical linear programming to determine which of the DMU under study form an envelopment surface. This envelopment surface is referred to as the efficient frontier. DEA provides a comprehensive analysis of relative efficiency for multiple input-multiple output situations by evaluating each DMU and measuring its performance relative to this envelopment surface. Units that lie on determine the surface are deemed efficient in DEA terminology. Units that do not lie on the surface are termed inefficient, and the analysis provides a measure of their relative inefficiency.  The following example illustrates the basic idea behind DEA Cooper 2000. Table 3 lists the performance of nine firms, each with two inputs and one output. Input 1 is the number of employees; Input 2 is operating expenses. The output variable is revenues. Fig. 1 plots the data from Table 3. All other things being equal, it is natural to judge firms that use fewer inputs per unit output as more efficient. Thus, for this set of performance measures, the line connecting C, D, and E is defined as the “efficient frontier.” This frontier should touch at least one point and all points are therefore on or above in this case this line. All the data points can be “enveloped” within the region enclosed by theTable 3. DEA ExampleFirmsABCDEFGHINumber of employees x147842565.56Operating expenses x233124242.52.5Revenues y111111111frontier line the horizontal line passing through C, and the vertical line through E. Hence, the name data envelopment analysis.   The relative inefficiency of firms not on the frontier can be quantified. For example, the inefficiency of Firm “A” is shown in Fig. 1. The line OA is drawn and crosses the efficient frontier line at P. Thus, by measuring lines OP and OA, the efficiency of Firm A can be calculated as: OP/OA=0.8571. Thus, Firm A is only 86% as efficient as the leading firms it is competing against.   The analysis can be extended to identify improvements by comparing inefficient behaviors to the efficient frontier. The values needed for inefficient firms to become efficient can be calculated. In Fig. 1, for example, Firm A would move to the efficient frontier Point P if its employee/revenue ratio decreased to 3.4 while its expense/revenue ratio declined to 2.6. In the same sense all other things being equal, Firm B can move to the efficient frontier by movement to Point Q at 4.4,1.9.   DEA owes its popularity to three inherent powerful features. First, it has the ability to incorporate multiple inputs and multiple outputs—particularly when it is used in conjunction with linear programming. Linear programming can handle large numbers of variables and relations constraints. Second, DEA has no a priori assumptions. There is no need to assign weights to the different inputs and outputs. The weights are derived directly from the data, freeing the user from arbitrary, subjective weightings. Third, the measurement units of the different inputs and outputs need not be congruent. Some may involve the number of persons, areas of floor space, money expended, etc. The various scaling adjustments required for graphical purposes do not affect the relationships among the variables themselves in any way.Data Collection, Analysis, and Model OutputData CollectionThe proposed benchmarking models were developed using data collected from currently active construction companies. The data for this research were collected through a survey questionnaire that was divided into three parts. The first part, respondent information, collected general information about the person completing the survey. The second part, firm general information, gathered data related to firm type i.e., general contractor, construction management firm, subcontractor, other, firm industry sector i.e., commercial, industrial, etc., and the approximate rev-Fig. 2. Types of firms participating in surveyenues of the firm. Part Three of the survey questionnaire, firm overall performance, collected information about the performance of the firm on a company-wide basis.   Firms are asked to supply their schedule performance, cost performance, safety performance, customer satisfaction, profit, expenses on safety, and expenses on project management.   Five hundred and forty-five managers were asked to fill out and submit the survey. Of the 545 managers that received surveys, the research team received 88 responses, a 16.15% response rate. The 88 respondents represented 74 firms some firms submitted multiple responses. Fig. 2 shows the types of participating firms i.e., general contractor, construction management firm, subcontractor, design/build. Fifty percent of the participating firms have annual revenues exceeding $50 million.Data Analysis and Model OutputTable 4 provides summary performance statistics for the companies that responded to the survey. Although 74 different firms responded to the survey, not every company responding provided all the data requested. The table summarizes the data that were collected, and provides mean, standard deviation, minimum, maximum, 25th quartile, and 75th quartile metrics of performance. OHSA recordable incidence rate is not reported because only a few firms supplied it. These facts, along with the nonhomogeneous population due to the diverse nature of the 74 construction firms are a limitation of the research. As with any research project, more data would make the results more definitive.   The discriminatory power of any DEA model depends on the number of DMU used in the model and since each model depends on a different set of inputs, the number of DMU available to each model is dependent on the number of firms that provided all of the information required for that particular model. Further, theMetricNumber of firmsMeanSdMin.Max. 25th quartile75th quartileSchedule performance %6980.323.12101007595Cost performance %6981.7515.22101007590Customer satisfaction %6760.3425.25101004080EMR350.7020.13560.450.950.610.8Profit %233.0662.6450.14121.53.875Safety expenses %430.981.3440.150.251Project management expenses %404.3113.2611226Table 4. Metrics of Performance Descriptive StatisticsTable 5. Six Models’ Determination of Firm EfficiencyModelInputs+outputsNo.DMUNo.inefficient firmsFULL7215SCCPE62818SCCE53426SCCP44741SCC34441SC24645literature indicates that the minimum number of DMU in anymodel should be three times the number of variables Charnes 1978; 1981.   The data were processed through all seven models. Table 5 shows the number of variables inputs plus outputs, the number of firms providing the information required for that model, and the number of firms that the model deemed inefficient. Fig. 3 shows how the percentage of firms deemed inefficient rises as the model’s discriminatory power increases with more DMU and fewer variables.   The trade-off is that the models with the fewest variables are rating the companies using the least amount of information. Hence, intuitively, the most accurate model would be the model with the greatest number of variables in this case, the FULL model, if a sufficient number of DMU could be obtained for the model. Unfortunately, the FULL model only had 21 firms that answered the questions required for model usage.   Table 6 summarizes the DEA model with the highest number of variables as it was used to benchmark construction industry firm performance. This model is named FULL because it incorporates data for the 21 companies that provided sufficiently detailed data for all the metrics of output performance summarized in Table 2 and the input measures safety and project management expenses described earlier. Note that the FULL model, while utilizing the most information on each DMU of any model, barely has enough DMU to make the model work. As explained earlier, three times the number of variables are required for utilization 37 for FULL and exactly 21 companies answered enough questions to be used in the model. Table 7 shows the output for one company from the FULL model.Fig. 3. Models’ discriminatory power as relates to number of DMU and variables   Using the FULL model described in Table 5, the analysis identified efficient and less efficient firms. The benchmarking analysis scored each firm against a maximum score of 1.0. The most efficient firms achieved a score of 1.0, while less efficient firms scored less than 1.0. The analysis also calculated what specific performance metrics less efficient firms would achieve if their overall effectiveness was at leading practice levels. Thus, these calculations can serve to assist managers at less efficient firms by providing specific insight into how company performance can be improved.ConclusionsThe proposed benchmarking models address the limitations that have been identified in other benchmarking models previously developed. The models use the industry-relevant metrics most frequently identified in the literature as critical measures of the overall efficiency of a construction company. The proposed models allow construction firms to be evaluated on a company-wide basis and identify specific areas of improvement for individual firms. The models produce and report the magnitude of the deficit that a company must overcome in each of several key areas measured in order to become as efficient as the most efficient firms in the industry. Thus, the models can be used to alert managers of inefficient firms to areas within the company that require imme-Table 6. DEA Models to Benchmark PerformanceModel no.Model nameNumber of firmsInputsOutputs1FULL21• Project management expenses • Safety expenses• Scheduleperformance • Cost performance • Customersatisfaction • Profit• 1/EMRdiate attention if the company is to achieve a high level of overall efficiency and succeed long term in a volatile and increasingly competitive industry environment.  The proposed benchmarking models are developed using data collected from 74 firms, made up of general contractors, construction management firms, design/build firms, and subcontractors involved in residential, commercial, industrial, and heavy/highway construction. Fifty percent of the firms are of revenue size over $50 million.Additionally, research showed or accomplished the following:• DEA is applicable to benchmarking the company-wide efficiency of construction firms;• Though the models performed as expected, further analysisTable 7. FULL Benchmarking Model Output          DMU	Survey	Calculated	Percent No.	input/output	responses	metrics	Difference	%5	AN	0.51	Safety	0.25	0.115	−0.135	−54.00expenses %	PM	1.95	1	−0.95	−48.72expenses %Schedule	47.5	50	2.5	5.26 performance %Cost	65	70	5	7.69 performance %	1/EMR	1.31	1.37	0.05	4.11	Customer	65	82.5	17.5	26.92satisfaction %	Profit %	2.05	4.525	2.475	120.73with more data is recommended for validation;• The discriminatory power of the models increase with the number of DMU used to generate the model;• Significant progress was made in the identification of the number of variables metrics and DMU participating firms needed for an applied study of this type;• The proper metrics to consider for the model were identified based on an exhaustive search and analysis of the literature and determining a consensus. Sufficient data would make a definitive list easily obtainable;• Existing models were thoroughly critiqued and found to be inadequate for company-wide analysis. Of course each model is important and made enormous contributions. Each of the four existing models critiqued is very effective in measuring what they were designed to measure; and• As with nearly all research, more data will make the results more definitive, as well as making the model more accurate. Also, the diverse nature of the 74 construction firms results in a nonhomogeneous population for any statistical analysis.   The impact of applying models such as these industry-wide will be dramatic. As each individual firm in the industry uses the models’ results to improve itself, the overall level of productivity across the entire industry will also increase dramatically over time. Firms will complete projects more quickly, more safely, and less expensively.   Taxpayers, regulators, elected officials, and other key stakeholders will come to recognize the productivity advances in the industry, and will see that funds invested in construction maintenance and expansion projects are being spent efficiently and effectively. As a result, the industry as a whole will have increased credibility, and suggested projects will receive higher priorities in the battle for scarce public works and infrastructure funding.ReferencesBoynton, A. C., and Zmund, R. W. 1984. “An assessment of critical success factors.” Sloan Manage. Rev., 254, 17–27.Business Roundtable. 1982. “Improving construction safety performance: A construction industry cost effectiveness project report.” Report A-3, Reprinted 1991.Camp, R. 1989. Benchmarking: The search for industry best practices that lead to superior performance, ASQC Quality Press, Milwaukee.Charnes, A., Cooper, W. W., and Rhodes, E., 1978. “Measuring the efficiency of decision making units.” Eur. J. Oper. Res., 26, 429–444.Charnes, A., Cooper, W. W., and Rhodes, E. 1981. “Evaluating program and managerial efficiency: An application of data development analysis to program follow through.” Manage. Sci. 27, 668–697.Construction Best Practice Program CBPP. 1998. Key performance indicators. Project delivery and company performance, Construction Best Practice Program publication, Watford, U.K.Construction Industry Institute CII. 2000. “Benchmarking and metrics.” Data Report, Service Release 10.10.2000 CD-ROM, FormatConstruction Industry Institute, Univ. of Texas at Austin, Austin, Tex.Cooper, W. W., Seiford, L. M., and Tone, K. 2000. “Data envelopment analysis: A comprehensive text with models, applications, references, and DEA-solver software.” Kluwer Academic Publishers, Boston/ Dordrecht/London.El-Mashaleh, M. 2003. “Firm performance and information technology utilization in the construction industry: An empirical study.” Ph.D. dissertation, Univ. of Florida, Gainesville, Fla.El-Mashaleh, M. O’Brien, W. J., and Minchin, R. E. 2006. “Firm performance and information technology utilization in the construction industry.” J. Constr. Eng. Manage., 1325, 499–507.Farrell, M. 1957. “The measurement of productive efficiency.” J. R. Stat. Soc. Ser. A (Gen.), 1203, 253–281.Fisher, D., Miertschin, S., and Pollock, D. 1995. “Benchmarking in construction industry.” J. Manage. Eng., 111, 50–57. Hudson, D. 1997. “Benchmarking construction project execution.” Ph.D. dissertation. Univ. of Texas, Austin, Tex.Janssens, D. 1992. Design build explained, Macmillan, London.Jaselskis, E., Anderson, S., and Russell, J. 1996. “Strategies for achieving excellence in construction safety performance.” J. Constr. Eng. Manage., 1221, 61–70.Levitt, R., and Samelson, N. 1987. Construction safety management,McGraw-Hill, New York.McGeorge, D., and Palmer, A. 1997. Construction management: New directions, Blackwell Science Ltd., Oxford.McKim, R., Hegazy, T., and Attalah, M. 2000. “Project performance control in reconstruction projects.” J. Constr. Eng. Manage., 1262, 137–141.Rouse, P., Putterill, M., and Ryan, D. 1997. “Towards a general managerial framework for performance measurement: A comprehensive highway maintenance application.” J. Productivity Analysis, 82, 127–149.Sanvido, V., Grobler, F., Parfitt, K., Guvenis, M., and Coyle, M. 1992. “Critical success factors for construction projects.” J. Constr. Eng. Manage., 1181, 94–111.Spendolini, M. 1992. The benchmarking book, AMACOM, AmericanManagement Association, New York.Towill, R. 2001. “The idea of building business processes: The responsive housebuilder.” Constr. Manage. Econom., 19, 285–293.United States Air Force. 1995. The metrics handbook.Watkinson, M. 1992a. “Procurement alternatives.” Faculty of Building Journal, Nottingham, Autumn, 4–7.Watkinson, M. 1992b. “Procurement alternatives.” Faculty of Building Journal, Nottingham, Winter, 5–6.Watson, G. 1992. The benchmarking workbook: Adapting best practices for performance improvement, Productivity Press, Portland, Ore.10 / JOURNAL OF MANAGEMENT IN ENGINEERING © ASCE / JANUARY 2007 J. Manage. Eng., 2007, 23(1): 10-17 JOURNAL OF MANAGEMENT IN ENGINEERING © ASCE / JANUARY 2007 / 11 J. Manage. Eng., 2007, 23(1): 10-17 10 / JOURNAL OF MANAGEMENT IN ENGINEERING © ASCE / JANUARY 2007 J. Manage. Eng., 2007, 23(1): 10-17 