Innovation and performance in engineering designAmmon Salter & Richard TorbettTo cite this article: Ammon Salter & Richard Torbett (2003) Innovation and performance in engineering design, Construction Management and Economics, 21:6, 573-580, DOI:10.1080/0144619032000134101To link to this article:  https://doi.org/10.1080/0144619032000134101Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=rcme20
Construction Management and Economics (September 2003) 21, 573–580Innovation and performance in engineering designAMMON SALTER1* and RICHARD TORBETT281Programme on Innovation in the Built Environment, SPRU – Science and Technology Policy Research, University9 of Sussex, Mantell Building, Brighton BN1 9RF, UK1011	2Performance and Evaluation Team, Strategy Unit, Department of Trade and Industry, UK 12	Received 29 February 2000; accepted 7 March 200234567 This paper explores the experiences of engineering, design and construction organizations (EDOs) in devel8 oping innovative approaches to performance measurement in their design activities. It contrasts experiences 9 of these firms with those of manufacturing organizations. It finds that performance measurement of engi-2011 neering design activities is poorly understood in many industries, including construction. The development of new performance measures can provide a new opportunity for innovation in engineering, design and con-1 struction firms. At present, the measures used to assess design in construction are based on the financial2 performance of a project rather than other important objectives of the design process, such as quality and3 buildability. In manufacturing industries, firms rely on measures of new product development, but these 4 indicators also say little about the measurement of engineering design activities, that is, those on-going and 5 continuous. To realize the innovative potential of design and performance measures, EDOs need to take a 6 broader perspective on the nature of design activities, and to look to the experiences of leading manufac7 turing firms to find new ways of measuring and understanding their design activities.89	Keywords: Innovation, design, performance measurement, engineering, design and construction organizations,3011	manufacturing 123456789401112345678IntroductionThere is widespread consensus that design is becoming increasingly important in determining competitiveness. Design is critical for innovation because it involves the coupling between technical possibilities and market demands and opportunities (Freeman, 1983; Rothwell and Gardiner, 1983; Lorenz, 1986; Faulkner and Senker, 1995; Roy and Potter, 1996; Walsh, 1996; Roy and Riedel, 1997; Stankiewicz, 1998). In many manufacturing industries, most valueadded is now located in the design stage and not on the factory floor (Clark and Wheelright, 1992; Walsh, 1996).  Design has long been acknowledged by the innovation studies literature as being an important part of the innovation process (Freeman, 1983). Walsh (1996) argues that the design as a process is poorly under-*Author for correspondence. E-mail: a.j.salter@sussex.ac.ukstood in innovation studies. Designers often act as intermediaries or technological gatekeepers within manufacturing firms, moving among departments and accessing the information of specialized functions within the firm (Dumas and Whitfield, 1989). Studies of design in manufacturing have shown that there is extremely wide variation in what firms mean by the concept of design. When asked about design, firms include a large number of elements: fitness for use or performance; visual appearance; user friendliness; efficiency in production and use of materials; and safety or durability (Cohen et al., 1996). In this paper, we focus on engineering design.  We argue that current tools for measuring design performance focus almost exclusively on the new product development process for manufacturing industries. This paper provides case study evidence from engineering, design and construction organizations (EDOs) in the development of indicators of design performance. The paper contrasts EDOs with manufacturing firms toConstruction Management and EconomicsISSN 0144-6193 print/ISSN 1466-433X online © 2003 Taylor & Francis Ltd http://www.tandf.co.uk/journalsDOI: 10.1080/0144619032000134101
        learn about the development and adoption of performance measures for design activities. We outline a strategy for developing new performance measures of design activities in EDOs and linking these measures to the innovation process within the firm. We contend that for firms to realize the full innovative potential of performance measurement in design, firms need to look 8 beyond measures of the financial performance of the 9 project to explore the design process in the context of 1011 inter-project learning, client satisfaction and user needs.123	Research method45 The research was based on a three-part approach. The 6 first part involved a review of academic and trade liter7 ature on performance measurement in design activi8 ties. This review included how the engineering, design 9 and construction trade press view design performance 2011 indicators. Also, other publications, such as industry 1 reports, were examined. Although the focus of our 2 primary research was on the experiences of EDOs, an 3 important part of our work was a set of interviews with 4 academic and industrial experts on other manufactur5 ing industries. In particular, we were interested in con6 trasting the issues and methods used in design activities 7 in EDOs with those of other manufacturing industries. 8 The second part of the research involved interviews 9 and an audit of practices of collaborating organizations. 3011 The companies involved in this study were all major 1 engineering, design and construction organizations. 2 The sample of firms in our study was not statistically 3 representative of the entire population of the industry. 4 However, since our aim was to assess the ‘state-of-the5 art’ in design performance measurement our sample 6 was drawn from the largest, economically significant, 7 innovative firms in the sector. Most of the companies 8 were among the 500 largest engineering design firms 9 in the world, while others were among the largest con4011 tractors. In total, the 12 collaborators managed almost 1 $3 billion in design work yearly and employed close 2 to 30 000 engineers in their design activities. The 3 median firm in the group had $300 million in design 4 work with a staff of 3500.5 All the EDOs in our study were part of an industry6 association that acted as an unbiased broker among the organizations and helped to facilitate the research. The firms, in part, set the agenda themselves, with direction from the research team and industrial association. They helped to set the questions for the audit tool, the interviews, and the key issues that they wanted to see addressed in the research team’s work. Interestingly, there was a consensus among participants that it was necessary to share information about practices, problems and metrics. It could be argued thatthis level of user involvement in the research places biases on the findings. The research team found the collaborators to be genuinely interested in learning from each other and from the results of the study.  There was a wide-ranging, open discussion among participants about common problems and solutions. Normally, participants from the industrial collaborators were persons responsible for technical development within their organizations. In some cases, this would be the managing director or the head of R&D. All representatives were from senior management positions within their organizations. The group met for one day every two months over the two years.  The third part of the research involved the preparation of a series of industry studies and interviews with leading manufacturing firms. The purpose of this was to provide a snapshot of design performance measurement in other manufacturing industries in order to learn lessons for our main focus, the EDOs. In total, six case studies of leading manufacturing firms were developed on the basis of 15 interviews. The case studies were used for the preparation of the summary of developments in manufacturing for this paper. The sample of manufacturing firms was selected on the basis that they were leaders in their industry and they had extensive programmes to improve their design activities. This portion of the research involved several site visits to the design departments of manufacturing firms in the UK. To gain a fuller understanding of the differences between EDOs and manufacturing firms in design, further comparative research is required.Innovation and designInnovation often depends on the development of new organizational routines. These routines set the pattern of action within an organization and shape the behaviour of individuals (Tidd et al., 1997). Design activities can be an essential part of organizational routines for many firms, particularly EDOs. An understanding of performance in design is critical, not only for monitoring current practices but also for effectively shaping routines to promote innovation.  However, performance measurement alone cannot make a firm competitive or innovative. It was a common view among firms participating in our study to see performance measurement as a solution to their management problems. In our opinion, this view overemphasizes the importance of performance measures for innovation management. Often performance measures cover only the ‘easy’ or ‘simple’ activities that lend themselves to measurement or monitoring. Measurement will never be a substitute for innovation management. Managers need to weigh performance measures against their own judgement, intuition and experience. No single performance measure can provide solutions to problems of managing innovation in environments where there are complex patterns of interaction within and outside an organization. Performance measures can provide indicators about the activities of groups and individuals and, when these indicators are comple8 mented by feedback, discussion and debate, can 9 enhance management decision-making.10111Innovation and design in manufacturing23 Manufacturing firms have become increasingly inter4 ested in improving their design processes (see also 5 Busby, 1999; Pawar and Driva, 1999). Yet, they are 6 faced with a poor record of measurement of engineer7 ing design activities (Nixon, 1999). For example, wide8 spread industry surveys of research and development 9 (R&D) expenditures allow firms to benchmark their 2011 spending on R&D.1 No such figures are available about 1 design (Freeman, 1983). The evidence that is avail2 able suggests that design in firms is far less dependent 3 on size and sector than are research and development 4 expenditures. In other words, design is pervasive across 5 all sectors (Walsh, 1996).6 In manufacturing, engineering design activities often7 are not tied to particular departments. Design can be 8 carried out by a variety of departments within a firm. 9 The use of in-house and external design consultants 3011 varies between sectors. The main reasons for employ-1 ing consultant designers were a general lack of skill 2 within the firm and a desire to maintain a fresh flow 3 of ideas (Walsh, 1996). Walsh argues that since design 4 often crosses functional departments, few firms have 5 explicit strategies to improve design performance.6 Management teams have other responsibilities, and 7 design is not among them. Much design management 8 is silent, managed on a part-time basis through infor9 mal relations among interested parties across depart4011 ments (Faulkner and Senker, 1995; Walsh, 1996). 1 Also, designers often occupy a relatively low position 2 in the hierarchy of the firm (Nixon, 1999). Walsh 3 (1996) argues that because of this pattern of silent 4 management, design activities can be poorly managed 5 in manufacturing.6 The increasing importance of design for competitiveness has made some firms increase their effort to understand, measure and manage their design activities (Lester et al., 1998). These efforts have received mixed results. Pawar and Driva (1999) argued that few of the performance measures used by firms actually involve designers and developers. In new product development for manufacturing, the current stock of tools for controlling product development ‘is fragmented and only used in some parts of the product development process’, and cost dominates measures of performance. Overall product designers were dissatisfied with their performance measures for design and development activities, and were unable to see how to improve them without significant investments of time and money (Pawar and Driva, 1999, p. 63).Design in engineering, design and constructionIn contrast to these studies of design performance and innovation in manufacturing, few studies have been carried out on the performance of design activities in EDOs (Nicholson and Naamani, 1992; MacPherson et al., 1993; Roy and Potter, 1996; Veshosky, 1998). Yet, EDOs account for a significant portion of the economy. For example, EDOs based in the UK earned over $16 billion in income in 1996. They employed over 150 000 people, of whom 63 000 were full members of professional institutions (Davis Langdon Consultancy, 1997).  We suggest that the activities of these engineering designers differ from the conventional activities of designers involved in manufacturing. EDOs operate on the basis of projects, and projects are the predominant mode of production (Gann and Salter, 1998; Kabasakal et al., 1989; Winch and Scheider, 1993; Lansley, 1994; Morris, 1994; Shirazi et al., 1996). Manufacturing firms often do run projects, such as new product development, but these projects are usually located within the organization and are not the dominant form of the organization.  Manufacturing processes allow organizations the chance to develop sustained or continuous production and business processes. Innovation in manufacturing often involves the exploitation of economies of scale and scope (Chandler, 1990). In construction, team working, long term client relations and one-off or bespoke products characterize the production process.  In this project-based environment, design activities are often embedded in other activities, such as bidding. Interviewees suggested it was hard for them to disentangle the performance of design processes from other activities (see also Nixon, 1999, p. 816). In this respect, design activities in EDOs can be hidden in a forest of other actions. They argued that designers in the construction process rarely have the opportunity to ‘design’ the total process and then participate in effectively managing that process.2 Moreover, design is often separate from production, whereas in manufacturing, design and key production activities normally are integrated within the bounds of the firm.  EDOs have also come under increased pressure to justify fees to clients and contractors. Design organizations face more demanding clients and increased competition for work with ever-lower margins. For example, in the UK, clients and contractors have openly disregarded the tradition of devoting 7% of total project costs to design fees. Interviewees suggested that designers often lack the ability to demonstrate the benefits of their designs and services to the project. Innovativeness or cost-savings in design are subsumed into the totality of the project. The contribution of 8 designers is difficult to disentangle from the activities 9 of others, and therefore to price accurately. Moreover, 1011 according to interviewees, savings in design are notori1 ously hard to quantify and measure. In this environ2 ment of increased client pressure, design organizations 3 are beginning to look to performance measures as an 4 opportunity to show clients and contractors the value 5 and efficiency of their design activities.678	Design activities in engineering, design 9	and construction20111 As part of this study, we conducted interview-based 2 case studies of 12 leading EDOs in the UK and devel3 oped and implemented an audit of their current and 4 future practices. The audit was designed to compare 5 practices in design performance measurement both 6 among group members themselves and against firms 7 in the other industries. Interviews, face-to-face meet8 ings and documentation relating to design supple9 mented these responses.33011	Each firm was asked to list their three most impor-1 tant measures of design activities. The collaborators 2 listed a relatively large number of different indicators 3 in their design activities. Interviews suggested that 4 many of their current systems for measuring design 5 performance are unreliable. Most of the techniques 6 used to measure performance do not focus on design 7 in particular, but rather are focused on the project or 8 the firm as a whole. In this sense, EDOs face similar 9 problems to manufacturing firms in measuring design: 4011 they use a set of indicators that do not specifically 1 address design processes (Busby, 1999; Pawar and 2 Driva, 1999).3 Among group members, cost was the most common 4 technique used for measuring design performance. 5 Cost includes rate of return on the project together6 with cost of engineering time as a percentage of total project costs. One respondent suggested that there is often a strong correlation between commercial failure on a project and poor design performance: ‘Those projects which were not particularly successful are very often the ones where the design is not particularly good’. Yet, the opposite was not always true. Commercially successful projects were not always the best engineered. The respondent suggested there was a tendency to over-engineer when it was unnecessaryfor the client, in order to justify design fees.  Respondents listed several advantages to cost-based indicators. Chief among these advantages was that there was a strong incentive for the firm to collect these data. Most firms had rigorous and effective project finance systems that allowed for regular reviews of the cost of design. This information could be used to develop bids for new rounds of projects. The purchasing system of the firm often allows the design element to be managed separately. Cost-based indicators also permit income forecasting and an assessment of the financial viability of the project for the firm.  Interviewees argued that there were considerable disadvantages to cost-based indicators. Often design failure costs were not identified in the project financial system. Projects differ in their financial structure, and therefore they can be hard to compare. Cost-based indicators do not allow the firm to analyse the performance or completion of the design activities. The lessons learned from the project for the firm are not tracked, measured or reflected in the data. Cost-based indicators do not assess the quality of the designs. Rather, they reflect accuracy of cost estimates made at the beginning of the project. There is also a potential for inaccurate bookkeeping to spread the costs of failure between projects. Designers can hide their mistakes and failures across a range of projects.  One respondent argued that the dominance of financial indicators in the current environment limited the development of alternative management strategies. Improvements in cost-based indicators reflected improvements in the efficiency of the established routines of the firm. Interviewees argued that the measures did not help in developing new practices, and therefore they limited the potential for innovation within the firm. In this respect, cost-based indicators assess competence not innovative capability. They show whether the firm has achieved the targets set at the beginning of the project. They reflect how closely the firm is able to find a match between its expectation of performance and its actual performance on a particular project. As one interviewee commented, they ‘do not really help you ensure that your designs are particularly right’.  The dominance of cost-based measures in engineering, design and construction mirrors the findings from the manufacturing industry. Pawar and Driva (1999) found a similar focus on costs in their study of design performance measures for manufacturing industries.  The next most common techniques for measuring design performance cited by respondents were quality assurance practices and design reviews. All of the respondents have active quality assurance programmes that include design reviews, which vary in form among respondents. Experts within the firm regularly assess project teams’ work. Reviews may also draw in external experts, and provide regular objective measures of performance. They ensure that designs are carried out in an appropriate environment. They often require more effort to conduct than financial measures. They focus on the process of design to ensure that the methods used by the design team are ‘correct’ and 8 allow design teams to take corrective or even preven9 tive action in order to deal with problems that have 1011 developed in the course of the project. They force 1 designers to log their design activities and make them 2 justify their design decisions. They help to build teams 3 and tie together actors on a project. One interviewee 4 commented that they provide ‘fresh minds’ to prob5 lems; reviews can be an open forum for designers to 6 share ideas and deal with the ‘big issues’ of the project: 7 often they are ‘gritty, demanding and authoritative’.8 Yet, interviewees argued that quality assurance pro9 cedures and design reviews are closer to a procedure 2011 or process than a measure of performance. Although 1 reviews force designers to identify and address root 2 causes of design problems, often they can be a ‘paper 3 chase’ as designers go through the motions of follow4 ing procedures. Reviews often fail to identify ineffi5 ciencies in current processes. Respondents commented 6 that they limit innovation by forcing designers to follow 7 traditional methods. They suggested that reviews could 8 be partially non-productive. Reports about projects 9 often provide little new information and things ‘go into 3011 a file and stay there’. Other respondents argued that 1 they do not challenge designers to improve, and this 2 can lead to conservatism. In a similar study of design 3 organizations in manufacturing, Busby (1999) found 4 that design reviews were not always successful in catch5 ing errors or mistakes. They occurred too late, involved 6 too little effort, and were viewed suspiciously by those 7 being reviewed. They also tended to focus on negative 8 aspects of projects, rather than successful areas. This 9 helped to poison the views of designers towards the 4011 review process (Busby, 1999, p. 56).1 The third main technique used by group members2 for assessing design performance was time. Time as 3 an indicator was often tied to cost-based indicators. 4 Time was found to give designers early warning if a 5 project was not going well. It was often critical for6 project excellence and important for the firm’s strategy or reputation. Time-based indicators did not help rescheduling to achieve targets for design. Indicators between projects may not be comparable. Time may also slip for reasons outside the control of the design team, such as approval problems.  Client feedback mechanisms frequently play an important role in measuring design performance. Most of the firms said that client feedback is a key indicator for their design performance. Client feedback was seen to offer opportunities to identify successes and failures in the project. Feedback also points to key areas for improvement in firm practices. Several respondents made the comment that client feedback needs to be collected in a systematic manner, yet in their firm this was rarely the case. A number of firms were developing formal systems for measuring client satisfaction. One respondent integrated client feedback with postproject appraisals. If the client was present, they found that the meetings provided very clear signals about firm practices. The meetings were often healthy for the company and opened up the firm for debate. They could also be good for sales. However, this was rare. Generally, client feedback sessions were completed too late in the project to be useful, interviewees argued.  The role of feedback from users in design performance was also addressed in Busby’s (1999) study of design organizations in manufacturing. Busby found that designers received little feedback from users. The time lag between the design stage and use was too great for designers to be associated with particular design activities. Designers had little contact with users and customers (Busby, 1999, p. 57). This was similar to the results of the present study, in which we found that client feedback was not systematic. We also found that there was little interaction or feedback from users, who often differed from the clients over the particular piece of design work.  Each of the firms in the study was asked about the performance measures they were considering implementing in the next five years. Several firms suggested that they intended to develop better mechanisms for collecting data on customer satisfaction. Existing procedures tended to rely on sporadic information from either particularly satisfied or particularly dissatisfied customers. Interviewees argued that this was not as useful as knowing how customers felt about the standard performing projects.  Several respondents discussed new and more flexible financial systems that are about to be put in place within the firm. These systems seemed to bring benefits in the form of better intra-firm communication of existing data, rather than new types of indicator. It was suggested that financial measures, regardless of how efficiently distributed, will not be able to give a realistic impression of design process performance. However, one respondent argued that adaptations in their financial system would allow them to better identify the more routine design activities and develop measures for these routine activities. More detailed project control systems, linked to financial systems, would allow the firm to track project processes better. The systems would make it possible to incorporate financial information with more qualitative aspects of design performance.Value management approaches at the start of projects are also seen by some firms to be an important step forward. Under this scheme, rather than only having client feedback at the end of the project, by which time it is too late, performance criteria and key milestones were established in conjunction with the client at the outset of the project. The contractor had to sit down with the client and rigorously formalize the 8 client’s needs and the technical feasibility of the project 9 if this were to be successful.1011 Respondents also argued that new information and 1 communication technology systems would allow them 2 to track current design activities more effectively and 3 offer opportunities for standardization. The common 4 feeling among respondents was that designers have a 5 tendency to ‘re-invent the wheel’. By allowing the firm 6 to keep, track and store information and experience 7 from previous projects, IT systems could help to ensure 8 that designers use the past experience of the firm in 9 the current activities.2011 Improvements in the management of the design 1 process were also seen to be a way to improve per2 formance. One respondent felt that increasing the 3 amount and increasing the timeliness of peer review4 ing would improve design performance. Another 5 argued that requiring business plans within the firm to 6 include process improvements would be a source of 7 performance improvement.8 Respondents were asked to describe how perform9 ance measures are collected within the firm. 3011 Interestingly, methods for collecting data differed 1 among respondents. Some firms collected data project2 by-project and then passed the data to senior man3 agers for review. In other firms, data were collected 4 centrally. In most firms, a senior committee of the firm, 5 typically associated with the financial department, 6 reviewed the data. Pawar and Driva (1999) found a 7 similar situation in manufacturing, where data were 8 collected by one department and not shared with 9 others. Moreover, the performance data were rarely 4011 supplied to the designers themselves, and so they had 1 little opportunity to assess their own performance. We 2 found a similar pattern of activity in EDOs. Little infor3 mation or performance data were given to front-line 4 design staff, according to an interviewee.5 Respondents indicated how and when performance6 measurement data were collected. Almost all firms claimed that the data they collected came from all stages of the design process. Financial measures were seen to be more effective and reliable collection mechanisms than non-financial, process quality oriented measures. Employee incentives also were linked to financial performance measures rather than performance measures of design. Since incentives are linked to financial data, meeting financial targets often has a higher priority than quality oriented targets for designers. In many cases,where both financial and non-financial data were collected, they were used by two separate groups of people within the firm. There was only one firm where the project head coordinated the collection, analysis and use of both types of data. The integration of data collection and analysis was seen as a significant source of potential competitive advantage, yet this opportunity had not been fully exploited, according to our interviewees, given the time and resources required.  Respondents were asked to assess the importance of various uses of design performance data. The results of this assessment suggested that financial indicators on projects were the most common use of design performance data. Design performance data were used to check over work, measure group performance and for quality assurance. Notably, few respondents indicated that design performance data were used as a management tool or as part of the design strategy within the firm. This would suggest that design performance data are often not well integrated into an overall management approach to design performance, but rather the data are used to assess the financial performance of groups and the firm.  Respondents were asked what mechanisms they used to locate the position of their firm’s design performance in relation to other firms in the sector. In most cases, respondents did not have any formal mechanisms for this type of design benchmarking. Informal discussions with other organizations within the sector were common, but rarely systematic. These informal discussions took place at conferences and sectoral meetings and indeed whenever paths suitably crossed. Client feedback mechanisms were used to assess the performance of the firm in comparison with other firms in the sector. Clients were also used to gather informal impressions of the state of firm practices. One respondent used feedback after unsuccessful bids. Success rates in bidding were also used as an indicator. Several respondents relied on bid success rates as their only measure. Levels of industry recognition and awards, such as ‘consultant of the year’, were used to signal excellence within the practice. Working in joint ventures and with subcontractors also provided opportunities for assessing performance.  The interviewees were asked if there were any design performance measures they would like to have but were unavailable. One area mentioned by most respondents was quantitative measures of design quality, but this information was seen to be very difficult to collect. Reviews on competitors’ work on projects throughout the product lifecycle were also mentioned. Some respondents wanted to have a greater appreciation of the experience and capabilities of other departments within the company. They felt the size of the firm and tendency for functional silos to develop had limited interdepartmental learning. Future cost values and verification cost values were also mentioned in responses. There was a general feeling among respondents that the current information was unreliable and patchy. One respondent demanded ‘facts’, such as the percentage of time of rework, number of discovered errors, and number of escaped errors. Working with clients it might be possi8 ble, interviewees suggested, to develop indicators and to 9 assess these indicators throughout the life of the project. 1011 This was seen to be dependent on the willingness of the 1 client to engage in two-way communication.2 Client feedback mechanisms were also mentioned as 3 a possible source of more focused and objective cust4 omer information. Client feedback could also incorpor5 ate information on post-construction performance. One 6 respondent suggested it would be useful to have infor7 mation about the impact of the design on the total pro8 ject costs and project performance. At present, it was 9 difficult for the firm to disentangle design from other 2011 activities. One respondent suggested that it might be 1 useful for firms to find comparative values for elements 2 of the design process, such as administration and over3 head costs, and compare these costs among projects.456	Conclusions78Here we evaluate lessons about developing perform-9ance measures in design activities and integrating them3011into the innovation process within the firm.12 (1) There is over-reliance on cost-based measures 3 of design performance in EDOs. Most measures 4 are linked to the financial performance of the 5 entire project. Cost-based indicators do not 6 show the alternatives available to the design. 7 They tend to reinforce current practice and are 8 not capable of supplying the information nec9 essary for innovation within the firm. Inter4011 viewees argued that a new and more balanced 1 approach was required. This approach would 2 supplement financial indicators with measures 3 of quality and process.4 (2) Our study suggests that the development of per5 formance measures should be seen as a process,6 and not simply as a set of indicators. Performance measures should be seen as a part of firm strategy, and need to be integrated into an overall innovation strategy (Burns and Stalker, 1961; Peteraf, 1993; Nonaka, 1994; Neely et al., 1996).(3) The use of informal measures is extremely important in evaluating design performance. Important elements of the design process, such as knowledge and ideas, are hard to measure, but this should not lead to them being discounted.If there is a mismatch between performance measures and informal understanding, individuals will rebel against the ‘tyranny of numbers’.(4) Performance indicators work best when design (and related) staff are involved in the collection and interpretation of data. Yet frequently in many firms there are weak incentives for individuals to complete quality measures (Busby, 1999). Also, designers may have little access to or take little account of the performance measures used to assess their work. Greater interaction between designers and performance measures is required.  Although there are important differences between construction and manufacturing in the design process, there are many lessons that each sector can learn from the other. However, the simple transfer of manufacturing design measurements to the construction process would be impossible, and maybe even unwise. This paper offers an opportunity for a two-way interaction between manufacturing and EDOs in the development of performance measures for engineering design activities (Busby, 1999; Pawar and Driva, 1999; Nixon, 1999).  Although performance indicators have the power to shape behaviour and promote innovation, they should not be seen as an elixir for the problems of managing a firm based on projects (Gann and Salter, 1998). Performance measures can be expensive to develop, collect and analyse. They can generate resentment among the workforce. They need to be integrated into the innovation strategy of the firm and linked to overall company-level objectives as well as those of discrete projects.  Although preliminary and indicative, the study has found that the current reliance on financially based performance measures tied to projects is inadequate for design activities in both manufacturing and construction. A wider perspective is necessary. Firms should combine financial data with other measures of design, including quality, flair and safety. Designers need to be involved in the creation, implementation and analysis of their own performance. Those firms willing to look beyond financial indicators and seek to involve their designers in the measurement of their performance should be able to reap the benefits and improve their innovative potential.AcknowledgementsThe authors would like to thank all the firms who participated in this study for their assistance and support. The paper would not have been possible without the efforts of the industrial research association who commissioned the original study upon which this paper is based. The authors are grateful for discussions with David Gann, Mike Hobday, David Twigg, Paul Nightingale, Tim Venables and Stefano Brusoni, and for suggestions by the anonymous referees. Usual disclaimers apply.8	References91011 Burns T. and Stalker, G. M. (1961) The Management of 1 Innovation, Oxford University Press.2 Busby, J. S. (1999) Problems in error correction, learning 3 and knowledge of performance in design organizations. IIE 4 Transactions, 31, 49–59.5 Chandler, A. D. (1990) Scale and Scope: The Dynamics of Industrial Capitalism, The Belknap Press/Harvard Univer-6sity Press, Cambridge, MA.7Clark, K. and Wheelwright, S. (1992) Revolutionising Product8 Development, The Free Press, New York.9 Cohen, C., Walsh, V. and Richards, A. (1996) Learning by 2011 designer-user interactions. Paper presented at the COST 1 Final Conference on Management and New Technology, 2 Madrid, 12–14 June.3	Davis Langdon Consultancy (1997) Survey of UK Construc4	tion Professional Services, Department of the Environment, 5	Construction Industry Council, London.6 Dumas, A. and Whitfield, A. (1989) Why design is difficult to manage. European Journal of Management, 7(1), 50.7         Faulkner, W. and Senker, J. (1995) Knowledge Frontiers: 8           Public Sector Research and Industrial Innovation in Bio9 technology, Engineering, Ceramics, and Parallel Computing, 3011 Oxford University Press.1	Freeman, C. (1983) Design and British economic perform2	ance. Lecture, Design Centre, London.3 Gann, D. M. and Salter, A. (1998) Learning and innova4 tion management in project-based firms. Paper presented 5 at the 2nd International Conference on Technology Policy 6 and Innovation, Lisbon, 1998.7	Kabasakal, H., Sozen, Z. and Usdiken, B. (1989)Organizational context, structural attributes and manage-8ment systems in construction firms. Construction Manage-9ment and Economics, 7, 347–56.4011 Lansley, P. (1994) Analysing construction organizations. 1 Construction Management and Economics, 12, 337–48.2 Lester, R. K., Piore, M. J. and Malek, K. M. (1998) 3 Interpretive management: what general managers can learn 4 from design. Harvard Business Review, 4, March–April. 5 Lorenz, C. (1986) The Design Dimension: Product Strategy and 6 the Challenge of Global Marketing, Blackwell, Oxford.MacPherson, S. J., Kelly, J. R. and Webb, R. S. (1993) How designs develop: insights from case studies in building engineering services. Construction Management and Economics, 11, 475–85.Morris, P. (1994) The Management of Projects, Thomas Telford, London.Neely, A., Gregory, M. and Platts, K. (1995) Performance measurement system design: a literature review and research agenda. International Journal of Operations and Production Management, 15, 80–116.Nicholson, P. M. and Naamani, Z. (1992) Managing architectural design: a recent survey. Construction Management and Economics, 10, 479–87.Nixon, B. (1999) Evaluating design performance. International Journal of Technology Management, 17, 814–29.Nonaka, I. (1994) A dynamic theory of organizational knowledge creation. Organizational Science, 5, 15–37.Pawar, K. and Driva, H. (1999) Performance measurement for product design and development in a manufacturing environment. International Journal of Production Economics, 60, 61–8.Peteraf, M. (1993) The cornerstones of competitive advantage; a resource-based view. Strategic Management Journal, 14, 171–91.Rothwell, R. and Gardiner, J. (1983) The role of design in product and process change. Design Studies, 4(3), 161–9.Roy, R. and Potter, S. (1996) Managing engineering design in complex supply chains. International Journal of Technology Management, 12, 403–20.Roy, R. and Riedel, J. (1997) Design and innovation in successful production competition. Technovation, 17, 537–45. Shirazi, B., Langford, D. and Rowlinson, S. (1996) Organizational structures in the construction industry. Construction Management and Economics, 14, 199–212.Stankiewicz, R. (1998) The concept of ‘design space’. Mimeo, University of Lund.Tidd, J., Bessant, J. and Pavitt, K. (1997) Managing Innovation: Integrating Technological, Market and Organizational Change, Wiley, New York.Utterback, C. P. J. (1997) Multi-mode interaction among technologies. Research Policy, 26, 67–84.Veshosky, D. (1998) Managing innovation information in engineering and construction firms. Journal of Management in Engineering, 14, 58–66.Vincenti, W. (1990) What Engineers Know and How They Know It, Johns Hopkins University Press, Baltimore, MD.Walsh, V. (1996) Design, innovation and the boundaries of the firm. Research Policy, 25, 509–29.Winch, G. and Scheider, E. (1993) The strategic management of architectural practice. Construction Management and Economics, 11, 467–73.Winograd, T. (1996) Bringing Design in Software, ACM Press, New York.Womack, J. and Jones, D. (1996) Lean Thinking: Banish Waste and Create Wealth in Your Corporation, Simon & Schuster, New York.Notes1. Traditional definitions of R&D do include some aspectsof design activity but they tend to be focused on the experiences of manufacturers designing new products or processes. They do not account for ongoing processes of design associated within EDOs (Nixon, 1999).2. In some manufacturing industries, firms do not have theopportunity to design the total process.3. The three organizations that did not respond to the auditwere interviewed separately, and participated in workshops.	574	Salter and Torbett1111234567	Innovation and performance in engineering design	575111123456778950111234551178950111234551111112345679501112345511